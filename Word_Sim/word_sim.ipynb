{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\tanve\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torchtext\\vocab\\__init__.py:4: UserWarning: \n",
      "/!\\ IMPORTANT WARNING ABOUT TORCHTEXT STATUS /!\\ \n",
      "Torchtext is deprecated and the last released version will be 0.18 (this one). You can silence this warning by calling the following at the beginnign of your scripts: `import torchtext; torchtext.disable_torchtext_deprecation_warning()`\n",
      "  warnings.warn(torchtext._TORCHTEXT_DEPRECATION_MSG)\n",
      "c:\\Users\\tanve\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torchtext\\utils.py:4: UserWarning: \n",
      "/!\\ IMPORTANT WARNING ABOUT TORCHTEXT STATUS /!\\ \n",
      "Torchtext is deprecated and the last released version will be 0.18 (this one). You can silence this warning by calling the following at the beginnign of your scripts: `import torchtext; torchtext.disable_torchtext_deprecation_warning()`\n",
      "  warnings.warn(torchtext._TORCHTEXT_DEPRECATION_MSG)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from tqdm import tqdm\n",
    "from ELMO import ELMo\n",
    "import torch.nn.functional as F\n",
    "from sts_loader import STSDataset, wo_ELMO_Dataset\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "torch.manual_seed(42)\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load vocabularies\n",
    "vocab = torch.load('../hin_word_vocab.pt')\n",
    "character_vocab = torch.load('../hin_char_vocab.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tanve\\AppData\\Local\\Temp\\ipykernel_10072\\2963459868.py:6: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  scores = torch.tensor(scores, dtype=torch.float32)\n"
     ]
    }
   ],
   "source": [
    "# Get Sentences and Scores\n",
    "\n",
    "path = '../sts-train-hi.tsv'\n",
    "sts_dataset = STSDataset(path)\n",
    "s1, s2, scores = sts_dataset.format(char_vocab=character_vocab)\n",
    "scores = torch.tensor(scores, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataloader(s1, s2, scores, batch_size):\n",
    "    zipped = list(zip(s1, s2, scores))\n",
    "    dataloader = DataLoader(zipped, batch_size=batch_size, shuffle=True)\n",
    "    return dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make the dataloader\n",
    "s1_val = s1[:1000]\n",
    "s2_val = s2[:1000]\n",
    "scores_val = scores[:1000]\n",
    "batch_size = 64\n",
    "val_dataloader = create_dataloader(s1_val, s2_val, scores_val, batch_size)\n",
    "train_dataloader = create_dataloader(s1[1000:], s2[1000:], scores[1000:], batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tanve\\AppData\\Local\\Temp\\ipykernel_10072\\1386384247.py:5: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  scores_wo_elmo = torch.tensor(scores_wo_elmo, dtype=torch.float32)\n"
     ]
    }
   ],
   "source": [
    "# make another dataloader for the model without elmo using wo_ELMO_Dataset\n",
    "wo_elmo_dataset = wo_ELMO_Dataset(path, vocab)\n",
    "\n",
    "s1_wo_elmo, s2_wo_elmo, scores_wo_elmo = wo_elmo_dataset.format()\n",
    "scores_wo_elmo = torch.tensor(scores_wo_elmo, dtype=torch.float32)\n",
    "s1_wo_elmo_val = s1_wo_elmo[:1000]\n",
    "s2_wo_elmo_val = s2_wo_elmo[:1000]\n",
    "scores_wo_elmo_val = scores_wo_elmo[:1000]\n",
    "wo_elmo_val_dataloader = create_dataloader(s1_wo_elmo_val, s2_wo_elmo_val, scores_wo_elmo_val, batch_size)\n",
    "s1_wo_elmo_train = s1_wo_elmo[1000:]\n",
    "s2_wo_elmo_train = s2_wo_elmo[1000:]\n",
    "scores_wo_elmo_train = scores_wo_elmo[1000:]\n",
    "wo_elmo_train_dataloader = create_dataloader(s1_wo_elmo_train, s2_wo_elmo_train, scores_wo_elmo_train, batch_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ELMo(cnn_config = {'character_embedding_size': 16, \n",
    "                           'num_filters': 32, \n",
    "                           'kernel_size': 5, \n",
    "                           'max_word_length': 10, \n",
    "                           'char_vocab_size': character_vocab.num_chars}, \n",
    "             elmo_config = {'num_layers': 3,\n",
    "                            'word_embedding_dim': 150,\n",
    "                            'vocab_size': vocab.num_words}, \n",
    "             char_vocab_size = character_vocab.num_chars).to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(torch.load('../model_elmo_hindi.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_embedding_dim = 300\n",
    "\n",
    "\n",
    "class SimilarityModel_ELMO(nn.Module):\n",
    "    def __init__(self, elmo):\n",
    "        super(SimilarityModel_ELMO, self).__init__()\n",
    "        self.elmo = elmo        \n",
    "        self.lambdas = nn.Parameter(torch.randn(3))\n",
    "        self.lstm = nn.LSTM(word_embedding_dim, word_embedding_dim//2, batch_first=False, bidirectional=True, num_layers=2)\n",
    "\n",
    "        for param in self.elmo.parameters():\n",
    "            param.requires_grad = False\n",
    "        \n",
    "    def forward(self, sentence1, sentence2):\n",
    "        forward_output1, backward_output1, final_embeddings1 = self.elmo(sentence1)\n",
    "        encoding1 = torch.zeros_like(final_embeddings1[0])\n",
    "        for i in range(3):\n",
    "            encoding1 += self.lambdas[i] * final_embeddings1[i]\n",
    "\n",
    "        forward_output2, backward_output2, final_embeddings2 = self.elmo(sentence2)\n",
    "        encoding2 = torch.zeros_like(final_embeddings2[0])\n",
    "        for i in range(3):\n",
    "            encoding2 += self.lambdas[i] * final_embeddings2[i]\n",
    "        \n",
    "        # print(final_embeddings1[0].shape, final_embeddings2[0].shape, encoding1.shape, encoding2.shape)\n",
    "        # print the embeddings of the first sentence\n",
    "        # print(sentence1.shape, sentence2.shape)\n",
    "\n",
    "        lstm_out1, (hidden1, cell1) = self.lstm(encoding1)\n",
    "        lstm_out2, (hidden2, cell2) = self.lstm(encoding2)\n",
    "        # print(lstm_out1.shape, lstm_out2.shape)\n",
    "        last_output1 = lstm_out1[:, -1, :]\n",
    "        last_output2 = lstm_out2[:, -1, :]\n",
    "        # print(lstm_out1.shape, lstm_out2.shape)\n",
    "        lstm_out1 = lstm_out1.view(lstm_out1.size(0), -1)  \n",
    "        lstm_out2 = lstm_out2.view(lstm_out2.size(0), -1)  \n",
    "\n",
    "        # Compute the cosine similarity between the reshaped tensors\n",
    "        cos_sim = (F.cosine_similarity(lstm_out1, lstm_out2, dim=1) + 1)*5/2\n",
    "        # print((F.cosine_similarity(lstm_out1, lstm_out2, dim=0) + 1)*5/2)\n",
    "        return cos_sim\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_embedding_dim = 300\n",
    "\n",
    "\n",
    "class SimilarityModel_wo_ELMO(nn.Module):\n",
    "    def __init__(self, vocab_size, num_layers=2, bidirectional=True):\n",
    "        super(SimilarityModel_wo_ELMO, self).__init__()\n",
    "        # Define the embedding layer\n",
    "        self.embedding = nn.Embedding(vocab_size, word_embedding_dim)\n",
    "        \n",
    "        # Define the LSTM layer\n",
    "        self.lstm = nn.LSTM(word_embedding_dim, word_embedding_dim//2, num_layers=num_layers,\n",
    "                            bidirectional=bidirectional, batch_first=True)\n",
    "        \n",
    "    def forward(self, sentence1, sentence2):\n",
    "        # Convert sentence1 and sentence2 from lists of token IDs to tensors\n",
    "        sentence1 = torch.tensor(sentence1, dtype=torch.long)\n",
    "        sentence2 = torch.tensor(sentence2, dtype=torch.long)\n",
    "        \n",
    "        # Compute embeddings for each sentence\n",
    "        embedding1 = self.embedding(sentence1)\n",
    "        embedding2 = self.embedding(sentence2)\n",
    "        # print(embedding1.shape, embedding2.shape)\n",
    "        # Process embeddings with LSTM\n",
    "        lstm_out1, _ = self.lstm(embedding1)\n",
    "        lstm_out2, _ = self.lstm(embedding2)\n",
    "        # print(lstm_out1.shape, lstm_out2.shape)\n",
    "        # Use the final hidden state of each LSTM as sentence representations\n",
    "        last_output1 = lstm_out1[:, -1, :]\n",
    "        last_output2 = lstm_out2[:, -1, :]\n",
    "        \n",
    "        # Compute cosine similarity between the final outputs\n",
    "        cos_sim = F.cosine_similarity(last_output1, last_output2, dim=1)\n",
    "        \n",
    "        # Optionally, transform the cosine similarity to a different range (you can adjust as needed)\n",
    "        # cos_sim_transformed = (cos_sim + 1) * 5/2\n",
    "        \n",
    "        return cos_sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# similarity_model = SimilarityModel(model)\n",
    "similarity_model_ELMO = SimilarityModel_ELMO(model).to(device)\n",
    "similarity_model_wo_ELMO = SimilarityModel_wo_ELMO(vocab.num_words).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 10\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(similarity_model_ELMO.parameters(), lr=0.001)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 31%|███       | 23/75 [00:10<00:23,  2.24it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 16\u001b[0m\n\u001b[0;32m     14\u001b[0m s1 \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mstack(s1, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m     15\u001b[0m s2 \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mstack(s2, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m---> 16\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43msimilarity_model_ELMO\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43ms2\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;66;03m# print(outputs.squeeze().shape, scores.shape)\u001b[39;00m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;66;03m# print(outputs.shape, scores.shape)\u001b[39;00m\n\u001b[0;32m     19\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(outputs, scores)\n",
      "File \u001b[1;32mc:\\Users\\tanve\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\tanve\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[10], line 15\u001b[0m, in \u001b[0;36mSimilarityModel_ELMO.forward\u001b[1;34m(self, sentence1, sentence2)\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, sentence1, sentence2):\n\u001b[1;32m---> 15\u001b[0m     forward_output1, backward_output1, final_embeddings1 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43melmo\u001b[49m\u001b[43m(\u001b[49m\u001b[43msentence1\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     16\u001b[0m     encoding1 \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros_like(final_embeddings1[\u001b[38;5;241m0\u001b[39m])\n\u001b[0;32m     17\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m3\u001b[39m):\n",
      "File \u001b[1;32mc:\\Users\\tanve\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\tanve\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\tanve\\OneDrive\\Desktop\\INLP\\INLP-Project-ELMo-main\\ELMo\\STS\\ELMO.py:73\u001b[0m, in \u001b[0;36mELMo.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     68\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m     69\u001b[0m     \u001b[38;5;66;03m# character cnn\u001b[39;00m\n\u001b[0;32m     70\u001b[0m     \u001b[38;5;66;03m# convert x to tensor\u001b[39;00m\n\u001b[0;32m     71\u001b[0m     \u001b[38;5;66;03m# x = torch.stack(x, dim=0)\u001b[39;00m\n\u001b[0;32m     72\u001b[0m     x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m---> 73\u001b[0m     x \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchar_cnn(word) \u001b[38;5;28;01mfor\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m x]\n\u001b[0;32m     74\u001b[0m     \u001b[38;5;66;03m# lstm1\u001b[39;00m\n\u001b[0;32m     75\u001b[0m     x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mstack(x, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m) \n",
      "File \u001b[1;32mc:\\Users\\tanve\\OneDrive\\Desktop\\INLP\\INLP-Project-ELMo-main\\ELMo\\STS\\ELMO.py:73\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     68\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m     69\u001b[0m     \u001b[38;5;66;03m# character cnn\u001b[39;00m\n\u001b[0;32m     70\u001b[0m     \u001b[38;5;66;03m# convert x to tensor\u001b[39;00m\n\u001b[0;32m     71\u001b[0m     \u001b[38;5;66;03m# x = torch.stack(x, dim=0)\u001b[39;00m\n\u001b[0;32m     72\u001b[0m     x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m---> 73\u001b[0m     x \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchar_cnn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mword\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m x]\n\u001b[0;32m     74\u001b[0m     \u001b[38;5;66;03m# lstm1\u001b[39;00m\n\u001b[0;32m     75\u001b[0m     x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mstack(x, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m) \n",
      "File \u001b[1;32mc:\\Users\\tanve\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\tanve\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\tanve\\OneDrive\\Desktop\\INLP\\INLP-Project-ELMo-main\\ELMo\\STS\\ELMO.py:34\u001b[0m, in \u001b[0;36mCharCNN.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     31\u001b[0m x \u001b[38;5;241m=\u001b[39m [conv(x) \u001b[38;5;28;01mfor\u001b[39;00m conv \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv_layers]\n\u001b[0;32m     33\u001b[0m \u001b[38;5;66;03m# now we max pool\u001b[39;00m\n\u001b[1;32m---> 34\u001b[0m x \u001b[38;5;241m=\u001b[39m [torch\u001b[38;5;241m.\u001b[39mmax(torch\u001b[38;5;241m.\u001b[39mrelu(conv), dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m conv \u001b[38;5;129;01min\u001b[39;00m x]\n\u001b[0;32m     36\u001b[0m \u001b[38;5;66;03m# now we concatenate the results\u001b[39;00m\n\u001b[0;32m     37\u001b[0m x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat(x, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m) \u001b[38;5;66;03m# (batch_size, num_filters * (max_word_length - kernel_size + 1))\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\tanve\\OneDrive\\Desktop\\INLP\\INLP-Project-ELMo-main\\ELMo\\STS\\ELMO.py:34\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     31\u001b[0m x \u001b[38;5;241m=\u001b[39m [conv(x) \u001b[38;5;28;01mfor\u001b[39;00m conv \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv_layers]\n\u001b[0;32m     33\u001b[0m \u001b[38;5;66;03m# now we max pool\u001b[39;00m\n\u001b[1;32m---> 34\u001b[0m x \u001b[38;5;241m=\u001b[39m [torch\u001b[38;5;241m.\u001b[39mmax(\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrelu\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconv\u001b[49m\u001b[43m)\u001b[49m, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m conv \u001b[38;5;129;01min\u001b[39;00m x]\n\u001b[0;32m     36\u001b[0m \u001b[38;5;66;03m# now we concatenate the results\u001b[39;00m\n\u001b[0;32m     37\u001b[0m x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat(x, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m) \u001b[38;5;66;03m# (batch_size, num_filters * (max_word_length - kernel_size + 1))\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "num_epochs = 20\n",
    "\n",
    "# train the model\n",
    "for epoch in range(num_epochs):\n",
    "    similarity_model_ELMO.train()\n",
    "    total_loss = 0\n",
    "    for s1, s2, scores in tqdm(train_dataloader):\n",
    "        s1 = s1\n",
    "        s2 = s2\n",
    "        scores = scores.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        # print(len(s1[0]))\n",
    "        # stack the sentences\n",
    "        s1 = torch.stack(s1, dim=1).to(device)\n",
    "        s2 = torch.stack(s2, dim=1).to(device)\n",
    "        outputs = similarity_model_ELMO(s1, s2)\n",
    "        # print(outputs.squeeze().shape, scores.shape)\n",
    "        # print(outputs.shape, scores.shape)\n",
    "        loss = criterion(outputs, scores)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    print(f\"Epoch {epoch} Loss: {total_loss}\")\n",
    "\n",
    "    similarity_model_ELMO.eval()\n",
    "    with torch.no_grad():\n",
    "        total_loss = 0\n",
    "        # calculate mean difference between predicted and actual scores\n",
    "        diff = 0\n",
    "\n",
    "        for s1, s2, scores in val_dataloader:\n",
    "            s1 = s1\n",
    "            s2 = s2\n",
    "            scores = scores.to(device)\n",
    "            s1 = torch.stack(s1, dim=1).to(device)\n",
    "            s2 = torch.stack(s2, dim=1).to(device)\n",
    "            outputs = similarity_model_ELMO(s1, s2)\n",
    "            loss = criterion(outputs, scores)\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            diff += torch.abs(outputs - scores).sum().item()\n",
    "            # calculate \n",
    "\n",
    "        print(f\"Validation Loss: {total_loss}\" + f\" Mean Difference: {diff/len(s1_val)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/75 [00:00<?, ?it/s]C:\\Users\\tanve\\AppData\\Local\\Temp\\ipykernel_10072\\1272999811.py:16: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  sentence1 = torch.tensor(sentence1, dtype=torch.long)\n",
      "C:\\Users\\tanve\\AppData\\Local\\Temp\\ipykernel_10072\\1272999811.py:17: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  sentence2 = torch.tensor(sentence2, dtype=torch.long)\n",
      "100%|██████████| 75/75 [00:02<00:00, 26.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 Loss: 393.2680969238281\n",
      "Validation Loss: 65.50843143463135 Mean Difference: 1.657077995300293\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 75/75 [00:02<00:00, 27.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 392.36709785461426\n",
      "Validation Loss: 65.43762755393982 Mean Difference: 1.657077995300293\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 75/75 [00:02<00:00, 27.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 393.5488407611847\n",
      "Validation Loss: 65.72825074195862 Mean Difference: 1.6570780029296874\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 75/75 [00:02<00:00, 27.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 393.09278297424316\n",
      "Validation Loss: 65.44252419471741 Mean Difference: 1.6570779724121094\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 75/75 [00:02<00:00, 28.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Loss: 393.126344203949\n",
      "Validation Loss: 66.07914996147156 Mean Difference: 1.657077995300293\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 75/75 [00:02<00:00, 26.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 Loss: 393.05983805656433\n",
      "Validation Loss: 65.79218029975891 Mean Difference: 1.6570779876708985\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 75/75 [00:02<00:00, 26.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6 Loss: 394.44569301605225\n",
      "Validation Loss: 65.99908709526062 Mean Difference: 1.657078010559082\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 75/75 [00:02<00:00, 26.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7 Loss: 392.3798532485962\n",
      "Validation Loss: 65.80016446113586 Mean Difference: 1.6570779876708985\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 53%|█████▎    | 40/75 [00:01<00:01, 26.04it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[18], line 19\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;66;03m# print(outputs.squeeze().shape, scores.shape)\u001b[39;00m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;66;03m# print(outputs.shape, scores.shape)\u001b[39;00m\n\u001b[0;32m     17\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(outputs, scores)\n\u001b[1;32m---> 19\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     20\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m     21\u001b[0m total_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[1;32mc:\\Users\\tanve\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\_tensor.py:522\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    512\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    513\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    514\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    515\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    520\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    521\u001b[0m     )\n\u001b[1;32m--> 522\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    523\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    524\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\tanve\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\autograd\\__init__.py:266\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    261\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    263\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    264\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    265\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 266\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    267\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    274\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "num_epochs = 20\n",
    "\n",
    "# train the model\n",
    "for epoch in range(num_epochs):\n",
    "    similarity_model_wo_ELMO.train()\n",
    "    total_loss = 0\n",
    "    for s1, s2, scores in tqdm(wo_elmo_train_dataloader):\n",
    "        s1 = s1.to(device)\n",
    "        s2 = s2.to(device)\n",
    "        scores = scores.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        # print(len(s1[0]))\n",
    "        # stack the sentences\n",
    "        outputs = similarity_model_wo_ELMO(s1, s2)\n",
    "        # print(outputs.squeeze().shape, scores.shape)\n",
    "        # print(outputs.shape, scores.shape)\n",
    "        loss = criterion(outputs, scores)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    print(f\"Epoch {epoch} Loss: {total_loss}\")\n",
    "\n",
    "    similarity_model_wo_ELMO.eval()\n",
    "    with torch.no_grad():\n",
    "        total_loss = 0\n",
    "        # calculate mean difference between predicted and actual scores\n",
    "        diff = 0\n",
    "\n",
    "        for s1, s2, scores in wo_elmo_val_dataloader:\n",
    "            s1 = s1.to(device)\n",
    "            s2 = s2.to(device)\n",
    "            # convert scores to float\n",
    "            \n",
    "            scores = scores.to(device)\n",
    "            outputs = similarity_model_wo_ELMO(s1, s2)\n",
    "            loss = criterion(outputs, scores)\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            diff += torch.abs(outputs - scores).sum().item()\n",
    "            # calculate \n",
    "\n",
    "        print(f\"Validation Loss: {total_loss}\" + f\" Mean Difference: {diff/len(s1_val)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
