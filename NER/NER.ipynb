{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from tqdm import tqdm\n",
    "import sys \n",
    "sys.path.append('../ELMo')\n",
    "from ELMO_used import ELMo\n",
    "import wandb\n",
    "import re\n",
    "import pandas as pd \n",
    "from preprocessing import tokenize, CharLevelVocab, WordLevelVocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "char_vocab = torch.load('../hindi_nli/Marathi_ELMo/char_vocab_marathi.pt')\n",
    "word_vocab = torch.load('../hindi_nli/Marathi_ELMo/word_vocab_marathi.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "elmo = ELMo(cnn_config = {'character_embedding_size': 16, \n",
    "                           'num_filters': 32, \n",
    "                           'kernel_size': 5, \n",
    "                           'max_word_length': 10, \n",
    "                           'char_vocab_size': char_vocab.num_chars}, \n",
    "             elmo_config = {'num_layers': 3,\n",
    "                            'word_embedding_dim': 150,\n",
    "                            'vocab_size': word_vocab.num_words}, \n",
    "             char_vocab_size = char_vocab.num_chars).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "elmo.load_state_dict(torch.load('../hindi_nli/Marathi_ELMo/elmo_marathi.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>words</th>\n",
       "      <th>labels</th>\n",
       "      <th>sentence_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>यादरम्यान</td>\n",
       "      <td>O</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>वर्षानुवर्षे</td>\n",
       "      <td>O</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>विसर्जनानंतर</td>\n",
       "      <td>O</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>स्वच्छता</td>\n",
       "      <td>O</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>करणाऱ्यांच्या</td>\n",
       "      <td>O</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199251</th>\n",
       "      <td>पोलिसांना</td>\n",
       "      <td>O</td>\n",
       "      <td>21501.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199252</th>\n",
       "      <td>क्वॉरंटाइन</td>\n",
       "      <td>O</td>\n",
       "      <td>21501.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199253</th>\n",
       "      <td>करण्यात</td>\n",
       "      <td>O</td>\n",
       "      <td>21501.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199254</th>\n",
       "      <td>आले</td>\n",
       "      <td>O</td>\n",
       "      <td>21501.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199255</th>\n",
       "      <td>होते</td>\n",
       "      <td>O</td>\n",
       "      <td>21501.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>199256 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                words labels  sentence_id\n",
       "0           यादरम्यान      O          1.0\n",
       "1        वर्षानुवर्षे      O          1.0\n",
       "2        विसर्जनानंतर      O          1.0\n",
       "3            स्वच्छता      O          1.0\n",
       "4       करणाऱ्यांच्या      O          1.0\n",
       "...               ...    ...          ...\n",
       "199251      पोलिसांना      O      21501.0\n",
       "199252     क्वॉरंटाइन      O      21501.0\n",
       "199253        करण्यात      O      21501.0\n",
       "199254            आले      O      21501.0\n",
       "199255           होते      O      21501.0\n",
       "\n",
       "[199256 rows x 3 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m_train_df = pd.read_table('train_iob.txt')\n",
    "m_train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[वर्षानुवर्षे, विसर्जनानंतर, स्वच्छता, करणाऱ्य...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[रैना, त्याला, मिळालेल्या, हॉटेलमधील, रुमबद्दल...</td>\n",
       "      <td>[BNEP, O, O, O, O, O, O]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[एकूणच, स्थलांतरितांच्या, भावना, स्पष्ट, करणार...</td>\n",
       "      <td>[O, O, O, O, O, O]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[ग्रामीण, मधील, ५०५, शहरातील, १९०, मालेगाव, शह...</td>\n",
       "      <td>[O, O, BNEM, O, BNEM, BNEL, O, BNEM, O, O, O]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[त्याला, अर्जुन, पुरस्कार, मिळाल्याची, बातमीही...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21488</th>\n",
       "      <td>[नवीन, फोनसाठी, ई-कॉमर्स, वेबसाइटवर, मायक्रो, ...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21489</th>\n",
       "      <td>[लसीच्या, घोषणेनंतर, जागतिक, कमॉडिटी, बाजारात,...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21490</th>\n",
       "      <td>[पुलवामा, हल्ल्यातील, आत्मघातकी, हल्लेखोराच्या...</td>\n",
       "      <td>[BNEL, O, O, O, O, O, O, O, O]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21491</th>\n",
       "      <td>[सुशांत, प्रियांकाविरुद्ध, रियाला, सांगताना, द...</td>\n",
       "      <td>[BNEP, BNEP, BNEP, O, O, O]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21492</th>\n",
       "      <td>[मास्क, सुरक्षित, असल्याचे, समजताच, या, मास्कच...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>21493 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                sentence  \\\n",
       "0      [वर्षानुवर्षे, विसर्जनानंतर, स्वच्छता, करणाऱ्य...   \n",
       "1      [रैना, त्याला, मिळालेल्या, हॉटेलमधील, रुमबद्दल...   \n",
       "2      [एकूणच, स्थलांतरितांच्या, भावना, स्पष्ट, करणार...   \n",
       "3      [ग्रामीण, मधील, ५०५, शहरातील, १९०, मालेगाव, शह...   \n",
       "4      [त्याला, अर्जुन, पुरस्कार, मिळाल्याची, बातमीही...   \n",
       "...                                                  ...   \n",
       "21488  [नवीन, फोनसाठी, ई-कॉमर्स, वेबसाइटवर, मायक्रो, ...   \n",
       "21489  [लसीच्या, घोषणेनंतर, जागतिक, कमॉडिटी, बाजारात,...   \n",
       "21490  [पुलवामा, हल्ल्यातील, आत्मघातकी, हल्लेखोराच्या...   \n",
       "21491  [सुशांत, प्रियांकाविरुद्ध, रियाला, सांगताना, द...   \n",
       "21492  [मास्क, सुरक्षित, असल्याचे, समजताच, या, मास्कच...   \n",
       "\n",
       "                                              labels  \n",
       "0                  [O, O, O, O, O, O, O, O, O, O, O]  \n",
       "1                           [BNEP, O, O, O, O, O, O]  \n",
       "2                                 [O, O, O, O, O, O]  \n",
       "3      [O, O, BNEM, O, BNEM, BNEL, O, BNEM, O, O, O]  \n",
       "4               [O, O, O, O, O, O, O, O, O, O, O, O]  \n",
       "...                                              ...  \n",
       "21488                       [O, O, O, O, O, O, O, O]  \n",
       "21489           [O, O, O, O, O, O, O, O, O, O, O, O]  \n",
       "21490                 [BNEL, O, O, O, O, O, O, O, O]  \n",
       "21491                    [BNEP, BNEP, BNEP, O, O, O]  \n",
       "21492                       [O, O, O, O, O, O, O, O]  \n",
       "\n",
       "[21493 rows x 2 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# make a df with sentence (list of words), labels (list of labels) based on the sentence_id\n",
    "# loop throughb the df, and append words to a temp sentence_list and labels to a temp label_list as long as sentence_id is same \n",
    "# append that to a new df and reset the temp lists\n",
    "# do this for the entire df\n",
    "\n",
    "def make_sentence_df(df):\n",
    "    sentence_list = []\n",
    "    labels_list = []\n",
    "    temp_sentence = []\n",
    "    temp_labels = []\n",
    "    for i in range(1, len(df)):\n",
    "        if df['sentence_id'][i] == df['sentence_id'][i-1]:\n",
    "            temp_sentence.append(df['words'][i])\n",
    "            temp_labels.append(df['labels'][i])\n",
    "        else:\n",
    "            sentence_list.append(temp_sentence)\n",
    "            labels_list.append(temp_labels)\n",
    "            temp_sentence = []\n",
    "            temp_labels = []\n",
    "    return pd.DataFrame({'sentence': sentence_list, 'labels': labels_list})\n",
    "\n",
    "sentence_m_train_df = make_sentence_df(m_train_df)\n",
    "sentence_m_train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = sentence_m_train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_df = pd.read_json('train.json')\n",
    "# train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['BNEO',\n",
       " 'INETI',\n",
       " 'BNEM',\n",
       " 'BED',\n",
       " 'BNEL',\n",
       " 'INEP',\n",
       " 'BNETI',\n",
       " 'IED',\n",
       " 'INEM',\n",
       " 'BNEP',\n",
       " 'INED',\n",
       " 'INEL',\n",
       " 'INEO',\n",
       " 'O',\n",
       " 'BNED']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get unique values in ner_tags column\n",
    "ner_tags = set()\n",
    "for tags in train_df['labels']:\n",
    "    for tag in tags:\n",
    "        ner_tags.add(tag)\n",
    "\n",
    "ner_tags = list(ner_tags)\n",
    "ner_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[चक्रवर्ती, खरंच, गायब, झाली, का]</td>\n",
       "      <td>[INEP, O, O, O, O]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[नियंत्रण, रेषेवर, जानेवारी, महिन्यात, दहशतवाद...</td>\n",
       "      <td>[O, O, BNED, O, O, O, O, O, O, O, O]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[फक्त, १४५००, मेगावॉटची, मागणी, आहे]</td>\n",
       "      <td>[O, BNEM, O, O, O]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[भवानी, पेठेतील, मुलीच्या, आईने, तक्रार, दिली,...</td>\n",
       "      <td>[BNEL, BNEL, O, O, O, O, O]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[साथीच्या, काळात, जवळपास, सर्वच, आमदार, खासदार...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1494</th>\n",
       "      <td>[प्रोसेसिंग, फी, आकारली, जात, नाही]</td>\n",
       "      <td>[O, O, O, O, O]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1495</th>\n",
       "      <td>[करोना, रुग्ण, बरे, होण्याचे, प्रमाण, ७१]</td>\n",
       "      <td>[O, O, O, O, O, BNEM]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1496</th>\n",
       "      <td>[विषाणूचा, मेंदूच्या, पेशींवर, परिणाम, होऊ, शक...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1497</th>\n",
       "      <td>[महापालिका, हद्दीतील, जास्तीज, जास्त, नागरिकां...</td>\n",
       "      <td>[INEO, O, O, O, O, O, O, O, O, O, O, O]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1498</th>\n",
       "      <td>[राजपूतच्या, कुटुंबियांनी, रियाने, सुशांतच्या,...</td>\n",
       "      <td>[INEP, O, BNEP, BNEO, O, BNEM, INEM, O, O, O, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1499 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               sentence  \\\n",
       "0                     [चक्रवर्ती, खरंच, गायब, झाली, का]   \n",
       "1     [नियंत्रण, रेषेवर, जानेवारी, महिन्यात, दहशतवाद...   \n",
       "2                  [फक्त, १४५००, मेगावॉटची, मागणी, आहे]   \n",
       "3     [भवानी, पेठेतील, मुलीच्या, आईने, तक्रार, दिली,...   \n",
       "4     [साथीच्या, काळात, जवळपास, सर्वच, आमदार, खासदार...   \n",
       "...                                                 ...   \n",
       "1494                [प्रोसेसिंग, फी, आकारली, जात, नाही]   \n",
       "1495          [करोना, रुग्ण, बरे, होण्याचे, प्रमाण, ७१]   \n",
       "1496  [विषाणूचा, मेंदूच्या, पेशींवर, परिणाम, होऊ, शक...   \n",
       "1497  [महापालिका, हद्दीतील, जास्तीज, जास्त, नागरिकां...   \n",
       "1498  [राजपूतच्या, कुटुंबियांनी, रियाने, सुशांतच्या,...   \n",
       "\n",
       "                                                 labels  \n",
       "0                                    [INEP, O, O, O, O]  \n",
       "1                  [O, O, BNED, O, O, O, O, O, O, O, O]  \n",
       "2                                    [O, BNEM, O, O, O]  \n",
       "3                           [BNEL, BNEL, O, O, O, O, O]  \n",
       "4                        [O, O, O, O, O, O, O, O, O, O]  \n",
       "...                                                 ...  \n",
       "1494                                    [O, O, O, O, O]  \n",
       "1495                              [O, O, O, O, O, BNEM]  \n",
       "1496               [O, O, O, O, O, O, O, O, O, O, O, O]  \n",
       "1497            [INEO, O, O, O, O, O, O, O, O, O, O, O]  \n",
       "1498  [INEP, O, BNEP, BNEO, O, BNEM, INEM, O, O, O, ...  \n",
       "\n",
       "[1499 rows x 2 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# repeat the process for validation and test data\n",
    "m_val_df = pd.read_table('valid_iob.txt')\n",
    "m_val_df\n",
    "\n",
    "sentence_m_val_df = make_sentence_df(m_val_df)\n",
    "\n",
    "\n",
    "val_df = sentence_m_val_df\n",
    "\n",
    "sentence_m_val_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[या, दहशतवादी, संघटनेने, या, हल्ल्याची, जबाबदा...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[पाहणी, करण्यासाठी, पहिल्यादांचा, ड्रोनचा, वाप...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[या, फोनमध्ये, २९, मेगापिक्सलचा, कॅमेरा, दिला,...</td>\n",
       "      <td>[O, O, BNEM, O, O, O, O]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[देशांमध्येही, फिलिपिनी, नर्सेस, खालोखाल, भारत...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[असतानाही, दुहेरी, हत्याकांड, घडले, आहे]</td>\n",
       "      <td>[O, O, O, O, O]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1993</th>\n",
       "      <td>[काही, दिवसांमध्ये, आयपीएलला, सुरुवात, होणार, ...</td>\n",
       "      <td>[O, O, O, O, O, O]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1994</th>\n",
       "      <td>[सुरक्षा, रक्षकांपासून, ते, पुजाऱ्यांपर्यंत, अ...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1995</th>\n",
       "      <td>[अन्य, नेटवर्कवर, कॉलिंग, साठी, या, प्लानमध्ये...</td>\n",
       "      <td>[O, O, O, O, O, O, BNEM, INEM, O, O, O]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1996</th>\n",
       "      <td>[या, अमेरिकी, यानाकडून, सातत्याने, मोहिती, पाठ...</td>\n",
       "      <td>[O, O, O, O, O, O, O]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1997</th>\n",
       "      <td>[आम्हाला, रोखू, शकत, नाही]</td>\n",
       "      <td>[O, O, O, O]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1998 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               sentence  \\\n",
       "0     [या, दहशतवादी, संघटनेने, या, हल्ल्याची, जबाबदा...   \n",
       "1     [पाहणी, करण्यासाठी, पहिल्यादांचा, ड्रोनचा, वाप...   \n",
       "2     [या, फोनमध्ये, २९, मेगापिक्सलचा, कॅमेरा, दिला,...   \n",
       "3     [देशांमध्येही, फिलिपिनी, नर्सेस, खालोखाल, भारत...   \n",
       "4              [असतानाही, दुहेरी, हत्याकांड, घडले, आहे]   \n",
       "...                                                 ...   \n",
       "1993  [काही, दिवसांमध्ये, आयपीएलला, सुरुवात, होणार, ...   \n",
       "1994  [सुरक्षा, रक्षकांपासून, ते, पुजाऱ्यांपर्यंत, अ...   \n",
       "1995  [अन्य, नेटवर्कवर, कॉलिंग, साठी, या, प्लानमध्ये...   \n",
       "1996  [या, अमेरिकी, यानाकडून, सातत्याने, मोहिती, पाठ...   \n",
       "1997                         [आम्हाला, रोखू, शकत, नाही]   \n",
       "\n",
       "                                       labels  \n",
       "0                    [O, O, O, O, O, O, O, O]  \n",
       "1                    [O, O, O, O, O, O, O, O]  \n",
       "2                    [O, O, BNEM, O, O, O, O]  \n",
       "3                 [O, O, O, O, O, O, O, O, O]  \n",
       "4                             [O, O, O, O, O]  \n",
       "...                                       ...  \n",
       "1993                       [O, O, O, O, O, O]  \n",
       "1994                 [O, O, O, O, O, O, O, O]  \n",
       "1995  [O, O, O, O, O, O, BNEM, INEM, O, O, O]  \n",
       "1996                    [O, O, O, O, O, O, O]  \n",
       "1997                             [O, O, O, O]  \n",
       "\n",
       "[1998 rows x 2 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test\n",
    "m_test_df = pd.read_table('test_iob.txt')\n",
    "\n",
    "sentence_m_test_df = make_sentence_df(m_test_df)\n",
    "\n",
    "test_df = sentence_m_test_df\n",
    "\n",
    "sentence_m_test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# val_df = pd.read_json('validation.json')\n",
    "# val_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get thee list of unique labels from the ner column in test_df and train_df\n",
    "labels = list(set([label for sublist in train_df.labels.tolist() for label in sublist] + [label for sublist in test_df.labels.tolist() for label in sublist] + [label for sublist in val_df.labels.tolist() for label in sublist]))\n",
    "\n",
    "# remove ' B-ORG'\n",
    "# labels.remove(' B-ORG')\n",
    "\n",
    "# labels = ['O', 'B-PER', 'I-PER', 'B-LOC', 'I-LOC', 'B-ORG', 'I-ORG']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['BNEO',\n",
       " 'INETI',\n",
       " 'BNEM',\n",
       " 'BED',\n",
       " 'BNEL',\n",
       " 'INEP',\n",
       " 'BNETI',\n",
       " 'IED',\n",
       " 'INEM',\n",
       " 'BNEP',\n",
       " 'INED',\n",
       " 'INEL',\n",
       " 'INEO',\n",
       " 'O',\n",
       " 'BNED']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # remove any tokens in the test_df and train_df which are not in the character_list.txt file \n",
    "# char_list = open('../ELMo/tamil_characters.txt').read().split('\\n')\n",
    "# char_list = [char for char in char_list if char != '']\n",
    "# # char_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def clean_tokens(tokens, acceptable_chars):\n",
    "#     cleaned_tokens = []\n",
    "#     for token in tokens:\n",
    "#         cleaned_token = re.sub(f'[^{acceptable_chars}]', '', token)\n",
    "#         if cleaned_token != '':\n",
    "#             cleaned_tokens.append(cleaned_token)\n",
    "#     return cleaned_tokens\n",
    "\n",
    "# # acceptable_chars = set(char_list)\n",
    "\n",
    "# train_df['tokens'] = train_df['words'].apply(lambda x: clean_tokens(x, acceptable_chars))\n",
    "# test_df['tokens'] = test_df['words'].apply(lambda x: clean_tokens(x, acceptable_chars))\n",
    "# val_df['tokens'] = val_df['words'].apply(lambda x: clean_tokens(x, acceptable_chars))\n",
    "\n",
    "# # # replace all instances of ' B-ORG' with 'B-ORG' in the ner column of test_df and train_df\n",
    "# # train_df['ner'] = train_df['ner'].apply(lambda x: [label.replace(' B-ORG', 'B-ORG') for label in x])\n",
    "# # test_df['ner'] = test_df['ner'].apply(lambda x: [label.replace(' B-ORG', 'B-ORG') for label in x])\n",
    "# # val_df['ner'] = val_df['ner'].apply(lambda x: [label.replace(' B-ORG', 'B-ORG') for label in x])\n",
    "\n",
    "# train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # add a column which contains the length of thr words column\n",
    "\n",
    "# train_df['token_length'] = train_df['tokens'].apply(lambda x: len(x))\n",
    "# train_df['words_length'] = train_df['words'].apply(lambda x: len(x))\n",
    "\n",
    "# train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rename the sentences column to tokens \n",
    "train_df = train_df.rename(columns = {'sentence': 'tokens', 'labels': 'ner_tags'})\n",
    "val_df = val_df.rename(columns = {'sentence': 'tokens', 'labels': 'ner_tags'})\n",
    "test_df = test_df.rename(columns = {'sentence': 'tokens', 'labels': 'ner_tags'})\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['BNEO',\n",
       " 'INETI',\n",
       " 'BNEM',\n",
       " 'BED',\n",
       " 'BNEL',\n",
       " 'INEP',\n",
       " 'BNETI',\n",
       " 'IED',\n",
       " 'INEM',\n",
       " 'BNEP',\n",
       " 'INED',\n",
       " 'INEL',\n",
       " 'INEO',\n",
       " 'O',\n",
       " 'BNED']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_sentences = train_df['tokens'].tolist()\n",
    "test_sentences = test_df['tokens'].tolist()\n",
    "val_sentences = val_df['tokens'].tolist()\n",
    "\n",
    "train_labels = train_df['ner_tags'].tolist()\n",
    "test_labels = test_df['ner_tags'].tolist()\n",
    "val_labels = val_df['ner_tags'].tolist()\n",
    "\n",
    "# change the train labels to their corresponding index in the labels list\n",
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21493 21493\n",
      "1998 1998\n",
      "1499 1499\n"
     ]
    }
   ],
   "source": [
    "print(len(train_sentences), len(train_labels))\n",
    "print(len(test_sentences), len(test_labels))\n",
    "print(len(val_sentences), len(val_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'BNEO': 0,\n",
       " 'INETI': 1,\n",
       " 'BNEM': 2,\n",
       " 'BED': 3,\n",
       " 'BNEL': 4,\n",
       " 'INEP': 5,\n",
       " 'BNETI': 6,\n",
       " 'IED': 7,\n",
       " 'INEM': 8,\n",
       " 'BNEP': 9,\n",
       " 'INED': 10,\n",
       " 'INEL': 11,\n",
       " 'INEO': 12,\n",
       " 'O': 13,\n",
       " 'BNED': 14,\n",
       " '<PAD>': 15,\n",
       " '<EOS>': 16,\n",
       " '<BOS>': 17,\n",
       " '<OOV>': 18}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels_to_idx = {label: idx for idx, label in enumerate(labels)}\n",
    "\n",
    "# add a pad_tag, end_tag, start_tag and oov_tag to the labels_to_idx dictionary\n",
    "labels_to_idx['<PAD>'] = len(labels_to_idx)\n",
    "labels_to_idx['<EOS>'] = len(labels_to_idx)\n",
    "labels_to_idx['<BOS>'] = len(labels_to_idx)\n",
    "labels_to_idx['<OOV>'] = len(labels_to_idx)\n",
    "\n",
    "labels_to_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change train_labels to their corresponding index using labels_to_idx \n",
    "train_labels = [[labels_to_idx[label] for label in sublist] for sublist in train_labels]\n",
    "test_labels = [[labels_to_idx[label] for label in sublist] for sublist in test_labels]\n",
    "val_labels = [[labels_to_idx[label] for label in sublist] for sublist in val_labels]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUT_OF_VOCAB = '<OOV>'\n",
    "PAD_TAG = '<PAD>'\n",
    "START_TAG = '<BOS>'\n",
    "END_TAG = '<EOS>'\n",
    "\n",
    "from preprocessing import tokenize, CharLevelVocab, WordLevelVocab\n",
    "\n",
    "class NERDataset(Dataset):\n",
    "    def __init__(self, sentences, labels, word_vocab: WordLevelVocab, char_vocab: CharLevelVocab, labels_to_idx: dict, max_word_length: int = 15, max_sentence_length: int = 15):\n",
    "        self.sentences = sentences # these are already cleaned and tokenised - only contained tokens with characters in the character_list.txt file\n",
    "        self.labels = labels\n",
    "        self.word_vocab = word_vocab\n",
    "        self.char_vocab = char_vocab\n",
    "        self.labels_to_idx = labels_to_idx\n",
    "        self.max_word_length = max_word_length\n",
    "        self.max_sentence_length = max_sentence_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sentences)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        sentence = self.sentences[idx]\n",
    "        return [torch.tensor([self.char_vocab.char_to_index(char) for char in word], dtype=torch.long) for word in sentence], torch.tensor(self.labels[idx], dtype=torch.long)\n",
    "\n",
    "    # def __getitem__(self, idx):\n",
    "    #     sentence = self.sentences[idx]\n",
    "    #     labels = self.labels[idx]\n",
    "\n",
    "    #     # convert the sentence to word and character indices\n",
    "    #     word_indices = self.word_vocab.word_to_index(sentence)\n",
    "    #     char_indices = self.char_vocab.word_to_index(sentence)\n",
    "\n",
    "    #     # pad the word and character indices\n",
    "    #     word_indices = self.pad_sequence(word_indices, self.word_vocab.word_to_index(PAD_TAG), self.max_sentence_length)\n",
    "    #     char_indices = [self.pad_sequence(word, self.char_vocab.char_to_index(PAD_TAG), self.max_word_length) for word in char_indices]\n",
    "    #     char_indices = self.pad_sequence(char_indices, [self.char_vocab.char_to_index(PAD_TAG)] * self.max_word_length, self.max_sentence_length)\n",
    "\n",
    "    #     # convert the labels to indices\n",
    "    #     labels = self.pad_sequence(labels, self.labels_to_idx[PAD_TAG], self.max_sentence_length)\n",
    "\n",
    "    #     return torch.tensor(word_indices), torch.tensor(char_indices), torch.tensor(labels)\n",
    "\n",
    "    def collate_fn(self, batch):\n",
    "        sentences, labels = zip(*batch)\n",
    "\n",
    "        bos_token = []\n",
    "        for c in START_TAG:\n",
    "            bos_token.append(self.char_vocab.char_to_index(c))\n",
    "        bos_token = torch.tensor(bos_token, dtype=torch.long)\n",
    "\n",
    "        eos_token = []\n",
    "        for c in END_TAG:\n",
    "            eos_token.append(self.char_vocab.char_to_index(c))\n",
    "        eos_token = torch.tensor(eos_token, dtype=torch.long)\n",
    "\n",
    "        pad_token = []\n",
    "        for c in PAD_TAG:\n",
    "            pad_token.append(self.char_vocab.char_to_index(c))\n",
    "        pad_token = torch.tensor(pad_token, dtype=torch.long)\n",
    "\n",
    "        sentences = [[bos_token] + list(sentence) + [eos_token] for sentence in sentences]\n",
    "\n",
    "        sentences = [sentence[:self.max_sentence_length] + [pad_token] * (self.max_sentence_length - len(sentence)) for sentence in sentences]\n",
    "\n",
    "        for i in range(len(sentences)):\n",
    "            for j in range(len(sentences[i])):\n",
    "                sentences[i][j] = torch.cat([sentences[i][j][:self.max_word_length], torch.tensor([self.char_vocab.char_to_index(PAD_TAG)]*(self.max_word_length - len(sentences[i][j])), dtype=torch.long)])\n",
    "        \n",
    "        sentences = torch.stack([torch.stack(sentence) for sentence in sentences])\n",
    "\n",
    "        labels = [[self.labels_to_idx[START_TAG]] + list(label) + [self.labels_to_idx[END_TAG]] for label in labels]\n",
    "\n",
    "        labels = [label[:self.max_sentence_length] + [self.labels_to_idx[PAD_TAG]] * (self.max_sentence_length - len(label)) for label in labels]\n",
    "        labels = torch.tensor(labels, dtype=torch.long)\n",
    "\n",
    "        return sentences, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([tensor([13,  4,  3, 36,  2, 11, 25, 13,  4,  3, 36,  5]),\n",
       "  tensor([13, 14, 12,  4,  3, 26, 11,  2, 11, 16,  6,  4]),\n",
       "  tensor([12,  3, 13, 19,  3, 66,  6,  2]),\n",
       "  tensor([ 9,  4, 20,  2, 61,  3,  7,  2, 16, 19,  3,  7,  2]),\n",
       "  tensor([15, 11,  2, 15, 34,  3,  7,  5]),\n",
       "  tensor([ 9,  4, 22, 11,  2, 15, 25, 35,  5]),\n",
       "  tensor([13,  2, 54, 10,  5, 10,  3,  7,  2]),\n",
       "  tensor([18,  3, 10,  2, 12,  3, 29, 14,  9]),\n",
       "  tensor([13,  2, 18,  4,  2, 19,  8]),\n",
       "  tensor([37,  8,  6,  8]),\n",
       "  tensor([23, 17,  5])],\n",
       " tensor([13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13]))"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ner_train_dataset = NERDataset(train_sentences, train_labels, word_vocab, char_vocab, labels_to_idx, max_word_length=10, max_sentence_length=15)\n",
    "ner_test_dataset = NERDataset(test_sentences, test_labels, word_vocab, char_vocab, labels_to_idx, max_word_length=10, max_sentence_length=15)\n",
    "ner_val_dataset = NERDataset(val_sentences, val_labels, word_vocab, char_vocab, labels_to_idx, max_word_length=10, max_sentence_length=15)\n",
    "\n",
    "#check the dataset\n",
    "ner_train_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([tensor([7, 2]),\n",
       "  tensor([21, 17, 28,  6, 13,  2, 21,  8]),\n",
       "  tensor([12, 16, 41, 29, 11,  5, 11,  5]),\n",
       "  tensor([7, 2]),\n",
       "  tensor([17, 10,  3, 10,  3,  7,  2, 19,  8]),\n",
       "  tensor([26, 32,  2, 32, 21,  2,  4,  8]),\n",
       "  tensor([41,  5,  6, 10,  8]),\n",
       "  tensor([17, 22,  6,  8])],\n",
       " tensor([13, 13, 13, 13, 13, 13, 13, 13]))"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ner_test_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make the dataloaders\n",
    "ner_train_dataloader = DataLoader(ner_train_dataset, batch_size=32, shuffle=True, collate_fn=ner_train_dataset.collate_fn)\n",
    "ner_test_dataloader = DataLoader(ner_test_dataset, batch_size=32, shuffle=True, collate_fn=ner_test_dataset.collate_fn)\n",
    "ner_val_dataloader = DataLoader(ner_val_dataset, batch_size=32, shuffle=True, collate_fn=ner_val_dataset.collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([[[82, 90, 83,  ...,  0,  0,  0],\n",
      "         [ 9, 56, 10,  ...,  4,  5,  0],\n",
      "         [15,  8,  0,  ...,  0,  0,  0],\n",
      "         ...,\n",
      "         [82, 87, 88,  ...,  0,  0,  0],\n",
      "         [82, 87, 88,  ...,  0,  0,  0],\n",
      "         [82, 87, 88,  ...,  0,  0,  0]],\n",
      "\n",
      "        [[82, 90, 83,  ...,  0,  0,  0],\n",
      "         [13, 14,  6,  ...,  0,  0,  0],\n",
      "         [ 9,  3, 36,  ...,  2,  6,  0],\n",
      "         ...,\n",
      "         [82, 91, 83,  ...,  0,  0,  0],\n",
      "         [82, 87, 88,  ...,  0,  0,  0],\n",
      "         [82, 87, 88,  ...,  0,  0,  0]],\n",
      "\n",
      "        [[82, 90, 83,  ...,  0,  0,  0],\n",
      "         [10,  9,  3,  ..., 13,  4,  0],\n",
      "         [ 1,  1, 83,  ...,  0,  0,  0],\n",
      "         ...,\n",
      "         [82, 87, 88,  ...,  0,  0,  0],\n",
      "         [82, 87, 88,  ...,  0,  0,  0],\n",
      "         [82, 87, 88,  ...,  0,  0,  0]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[82, 90, 83,  ...,  0,  0,  0],\n",
      "         [32,  5, 35,  ...,  0,  0,  0],\n",
      "         [26, 14, 10,  ...,  2,  6,  8],\n",
      "         ...,\n",
      "         [17, 22,  6,  ...,  0,  0,  0],\n",
      "         [82, 91, 83,  ...,  0,  0,  0],\n",
      "         [82, 87, 88,  ...,  0,  0,  0]],\n",
      "\n",
      "        [[82, 90, 83,  ...,  0,  0,  0],\n",
      "         [30, 11,  5,  ...,  0,  0,  0],\n",
      "         [15, 60, 28,  ...,  0,  0,  0],\n",
      "         ...,\n",
      "         [82, 87, 88,  ...,  0,  0,  0],\n",
      "         [82, 87, 88,  ...,  0,  0,  0],\n",
      "         [82, 87, 88,  ...,  0,  0,  0]],\n",
      "\n",
      "        [[82, 90, 83,  ...,  0,  0,  0],\n",
      "         [18, 14, 27,  ..., 29, 28, 11],\n",
      "         [33, 14, 12,  ...,  4,  0,  0],\n",
      "         ...,\n",
      "         [26,  2, 20,  ...,  0,  0,  0],\n",
      "         [82, 91, 83,  ...,  0,  0,  0],\n",
      "         [82, 87, 88,  ...,  0,  0,  0]]]), tensor([[17, 13, 13, 13, 13, 13, 13, 13, 16, 15, 15, 15, 15, 15, 15],\n",
      "        [17, 13, 13, 13, 13, 13, 13, 13, 14, 13, 13, 13, 16, 15, 15],\n",
      "        [17, 13,  0, 13, 13, 13, 13, 16, 15, 15, 15, 15, 15, 15, 15],\n",
      "        [17,  8, 13, 13, 13, 13, 13, 13, 16, 15, 15, 15, 15, 15, 15],\n",
      "        [17, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 16, 15, 15],\n",
      "        [17, 13, 13, 13, 13, 13, 13, 13, 13, 16, 15, 15, 15, 15, 15],\n",
      "        [17, 13, 13,  2, 13, 13,  4, 13,  2, 13, 13, 13, 16, 15, 15],\n",
      "        [17, 10, 13,  4,  2,  8,  8, 13, 13, 13, 16, 15, 15, 15, 15],\n",
      "        [17, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 16, 15, 15, 15],\n",
      "        [17, 13,  4, 11, 13, 13, 16, 15, 15, 15, 15, 15, 15, 15, 15],\n",
      "        [17, 11, 13, 13, 13, 13, 13, 13, 13, 13, 16, 15, 15, 15, 15],\n",
      "        [17, 13, 13,  4, 13, 13, 13, 13, 16, 15, 15, 15, 15, 15, 15],\n",
      "        [17,  4, 13, 13, 13, 13, 13, 13, 13, 13, 13, 16, 15, 15, 15],\n",
      "        [17, 13, 13,  4, 13, 13,  0, 13, 13, 13, 13, 13, 16, 15, 15],\n",
      "        [17, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 16, 15, 15, 15],\n",
      "        [17, 13, 13, 13, 13, 13, 13, 13, 13, 16, 15, 15, 15, 15, 15],\n",
      "        [17, 13, 13, 13, 13, 13, 13, 13, 16, 15, 15, 15, 15, 15, 15],\n",
      "        [17, 13, 13, 13, 13, 13, 13, 16, 15, 15, 15, 15, 15, 15, 15],\n",
      "        [17, 13,  2, 13, 13, 13, 13,  9, 13, 13, 13, 13, 13, 16, 15],\n",
      "        [17, 13,  9, 13, 13,  9, 13, 13, 13, 13,  9, 13, 16, 15, 15],\n",
      "        [17, 13, 13, 13,  4,  0, 13, 13, 16, 15, 15, 15, 15, 15, 15],\n",
      "        [17, 13, 13, 13, 13, 13, 13, 13, 16, 15, 15, 15, 15, 15, 15],\n",
      "        [17, 13, 13, 13, 13, 13, 13, 13, 13, 13, 16, 15, 15, 15, 15],\n",
      "        [17, 13, 13, 13, 13, 13, 13,  0,  3,  9, 13, 13, 13, 16, 15],\n",
      "        [17, 13, 13, 13, 13, 13, 13, 13, 13, 16, 15, 15, 15, 15, 15],\n",
      "        [17, 13,  4, 11, 13, 13, 13, 13, 16, 15, 15, 15, 15, 15, 15],\n",
      "        [17, 13, 13, 13, 13, 13, 16, 15, 15, 15, 15, 15, 15, 15, 15],\n",
      "        [17,  5,  9,  5, 13, 13, 13, 13, 13, 13, 16, 15, 15, 15, 15],\n",
      "        [17, 13, 13, 13, 13, 13, 16, 15, 15, 15, 15, 15, 15, 15, 15],\n",
      "        [17,  4, 13,  4, 13,  3,  9, 13, 13, 13, 13, 13, 13, 16, 15],\n",
      "        [17, 13, 13, 13, 13, 13, 13,  9, 13, 13, 16, 15, 15, 15, 15],\n",
      "        [17, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 16, 15]]))\n"
     ]
    }
   ],
   "source": [
    "# chekc if the dataloaders are working\n",
    "for batch in ner_train_dataloader:\n",
    "    print(batch)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NERModel(nn.Module):\n",
    "    def __init__(self, elmo, embedding_dim, hidden_dim_1, hidden_dim_2, num_classes):\n",
    "        super(NERModel, self).__init__()\n",
    "        self.elmo = elmo\n",
    "        self.lambdas = nn.Parameter(torch.randn(3))\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim_1, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim_1, hidden_dim_2)\n",
    "        self.output = nn.Linear(hidden_dim_2, num_classes)\n",
    "        self.non_linearity = nn.ReLU()\n",
    "\n",
    "        for param in self.elmo.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        forward_output, backward_output, final_embeddings = self.elmo(x)\n",
    "        # print the types\n",
    "        # print(f\"forward_output type: {type(forward_output)}\")\n",
    "        # print(f\"backward_output type: {type(backward_output)}\")\n",
    "        # print(f\"final_embeddings type: {type(final_embeddings)}\")\n",
    "\n",
    "        encoding = torch.zeros_like(final_embeddings[0])\n",
    "        # print(f\"encoding shape after initialization: {encoding.shape}\")\n",
    "\n",
    "        for i in range(3):\n",
    "            encoding += self.lambdas[i] * final_embeddings[i]\n",
    "        # print(f\"encoding shape after loop: {encoding.shape}\")\n",
    "\n",
    "        lstm_output, _ = self.lstm(encoding)\n",
    "        # print(f\"lstm_output shape: {lstm_output.shape}\")\n",
    "\n",
    "        fc_output = self.fc(lstm_output.contiguous().view(-1, lstm_output.shape[2]))\n",
    "        # print(f\"fc_output shape after fc layer: {fc_output.shape}\")\n",
    "\n",
    "        fc_output = self.non_linearity(fc_output)\n",
    "        # print(f\"fc_output shape after non-linearity: {fc_output.shape}\")\n",
    "\n",
    "        output = self.output(fc_output)\n",
    "        # print(f\"output shape: {output.shape}\")\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_ner(model, train_dataloader, val_dataloader, optimizer, criterion, num_epochs, device):\n",
    "    model.to(device)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        predictions = []\n",
    "        true_labels = []\n",
    "        val_predictions = []\n",
    "        val_true_labels = []\n",
    "        loss_val = 0\n",
    "\n",
    "        for batch in tqdm(train_dataloader):\n",
    "            sentences, labels = batch\n",
    "            sentences, labels = sentences.to(device), labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # print shapes of sentences and labels\n",
    "            # print(f\"sentences shape: {sentences.shape}\")\n",
    "            # print(f\"labels shape: {labels.shape}\")\n",
    "\n",
    "            output = model(sentences)\n",
    "\n",
    "            loss = criterion(output.view(-1, output.shape[-1]), labels.view(-1))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss += loss.item()\n",
    "            predictions.extend(output.argmax(dim=-1).view(-1).tolist())\n",
    "            true_labels.extend(labels.view(-1).tolist())\n",
    "\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for batch in tqdm(val_dataloader):\n",
    "                sentences, labels = batch\n",
    "                sentences, labels = sentences.to(device), labels.to(device)\n",
    "\n",
    "                output = model(sentences)\n",
    "\n",
    "                loss = criterion(output.view(-1, output.shape[-1]), labels.view(-1))\n",
    "                val_loss += loss.item()\n",
    "                val_predictions.extend(output.argmax(dim=-1).view(-1).tolist())\n",
    "                val_true_labels.extend(labels.view(-1).tolist())\n",
    "\n",
    "        wandb.log({'Train Loss': train_loss/len(train_dataloader), 'Val Loss': val_loss/len(val_dataloader)})\n",
    "        print(f'Epoch: {epoch+1}/{num_epochs}, Train Loss: {train_loss/len(train_dataloader)}, Val Loss: {val_loss/len(val_dataloader)}')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ner = NERModel(elmo, 300, 150, 100, len(labels_to_idx))\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model_ner.parameters(), lr=0.0005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mnandarajiv\u001b[0m (\u001b[33mproject-ai-scream\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.17.0 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home2/nanda.rajiv/inlp/project/new/INLP-Project-ELMo/tamil_ner/wandb/run-20240508_164534-gsvqxoke</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/project-ai-scream/INLP-Project-ELMo/runs/gsvqxoke' target=\"_blank\">Marathi with ELMo</a></strong> to <a href='https://wandb.ai/project-ai-scream/INLP-Project-ELMo' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/project-ai-scream/INLP-Project-ELMo' target=\"_blank\">https://wandb.ai/project-ai-scream/INLP-Project-ELMo</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/project-ai-scream/INLP-Project-ELMo/runs/gsvqxoke' target=\"_blank\">https://wandb.ai/project-ai-scream/INLP-Project-ELMo/runs/gsvqxoke</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 672/672 [00:48<00:00, 13.87it/s]\n",
      "100%|██████████| 47/47 [00:03<00:00, 14.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/10, Train Loss: 0.3123381589095862, Val Loss: 0.17282879796433956\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 672/672 [00:47<00:00, 14.25it/s]\n",
      "100%|██████████| 47/47 [00:03<00:00, 14.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2/10, Train Loss: 0.1491884539524714, Val Loss: 0.13357453571355088\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 672/672 [00:47<00:00, 14.10it/s]\n",
      "100%|██████████| 47/47 [00:03<00:00, 14.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3/10, Train Loss: 0.12144575171017398, Val Loss: 0.1274186706447855\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 672/672 [00:46<00:00, 14.34it/s]\n",
      "100%|██████████| 47/47 [00:03<00:00, 14.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4/10, Train Loss: 0.1053201984946749, Val Loss: 0.12017979615546287\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 672/672 [00:47<00:00, 14.20it/s]\n",
      "100%|██████████| 47/47 [00:03<00:00, 14.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5/10, Train Loss: 0.09376769582186603, Val Loss: 0.11414399013874378\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 672/672 [00:47<00:00, 14.28it/s]\n",
      "100%|██████████| 47/47 [00:03<00:00, 14.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 6/10, Train Loss: 0.08506280565190882, Val Loss: 0.11604854884616872\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 672/672 [00:47<00:00, 14.22it/s]\n",
      "100%|██████████| 47/47 [00:03<00:00, 14.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 7/10, Train Loss: 0.07707048350545977, Val Loss: 0.11721687082280503\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 672/672 [00:47<00:00, 14.24it/s]\n",
      "100%|██████████| 47/47 [00:03<00:00, 14.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 8/10, Train Loss: 0.07036440021779743, Val Loss: 0.11982634029489883\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 672/672 [00:47<00:00, 14.25it/s]\n",
      "100%|██████████| 47/47 [00:03<00:00, 13.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 9/10, Train Loss: 0.06420431323238604, Val Loss: 0.12143072374957672\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 672/672 [00:47<00:00, 14.28it/s]\n",
      "100%|██████████| 47/47 [00:03<00:00, 14.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 10/10, Train Loss: 0.058504576191938086, Val Loss: 0.12028853286136972\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Train Loss</td><td>█▄▃▂▂▂▂▁▁▁</td></tr><tr><td>Val Loss</td><td>█▃▃▂▁▁▁▂▂▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Train Loss</td><td>0.0585</td></tr><tr><td>Val Loss</td><td>0.12029</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">Marathi with ELMo</strong> at: <a href='https://wandb.ai/project-ai-scream/INLP-Project-ELMo/runs/gsvqxoke' target=\"_blank\">https://wandb.ai/project-ai-scream/INLP-Project-ELMo/runs/gsvqxoke</a><br/> View project at: <a href='https://wandb.ai/project-ai-scream/INLP-Project-ELMo' target=\"_blank\">https://wandb.ai/project-ai-scream/INLP-Project-ELMo</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240508_164534-gsvqxoke/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "config = {'model': 'ELMo', 'language': 'Marathi', 'epochs': 10, 'batch_size': 32, 'lr': 0.0005}\n",
    "\n",
    "wandb.init(project='INLP-Project-ELMo', group='NER', name='Marathi with ELMo', config=config)\n",
    "wandb.watch(model_ner)\n",
    "          \n",
    "train_ner(model_ner, ner_train_dataloader, ner_val_dataloader, optimizer, criterion, 10, device)\n",
    "\n",
    "wandb.join()\n",
    "\n",
    "# save the model\n",
    "torch.save(model_ner.state_dict(), 'ner_model_marathi_elmo.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_ner = NERModel(elmo, 300, 150, 100, len(labels_to_idx))\n",
    "# model_ner.load_state_dict(torch.load('ner_model_hindi_elmo.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/63 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 63/63 [00:04<00:00, 14.52it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.17.0 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home2/nanda.rajiv/inlp/project/new/INLP-Project-ELMo/tamil_ner/wandb/run-20240508_165627-wdu7mpqx</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/project-ai-scream/my_project/runs/wdu7mpqx' target=\"_blank\">my_run</a></strong> to <a href='https://wandb.ai/project-ai-scream/my_project' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/project-ai-scream/my_project' target=\"_blank\">https://wandb.ai/project-ai-scream/my_project</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/project-ai-scream/my_project/runs/wdu7mpqx' target=\"_blank\">https://wandb.ai/project-ai-scream/my_project/runs/wdu7mpqx</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/project-ai-scream/my_project/runs/wdu7mpqx?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x7f67a6386dc0>"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_ner.to(device)\n",
    "def eval_ner(model, test_dataloader, device):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(test_dataloader):\n",
    "            sentences, labels = batch\n",
    "            sentences, labels = sentences.to(device), labels.to(device)\n",
    "\n",
    "            # flatten labels \n",
    "            labels = labels.view(-1)\n",
    "\n",
    "            output = model(sentences)\n",
    "            # print(f\"output shape: {output.shape}\")\n",
    "            # print(f\"labels shape: {labels.shape}\")\n",
    "\n",
    "            output = output.argmax(dim=-1)\n",
    "\n",
    "            correct += (output == labels).sum().item()\n",
    "            total += labels.numel()\n",
    "\n",
    "    accuracy = correct/total\n",
    "    return correct/total\n",
    "    wandb.log({\"accuracy\": accuracy})\n",
    "\n",
    "accuracy = eval_ner(model_ner, ner_test_dataloader, device)\n",
    "accuracy\n",
    "wandb.init(project=\"my_project\", name=\"my_run\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUT_OF_VOCAB = '<OOV>'\n",
    "PAD_TAG = '<PAD>'\n",
    "START_TAG = '<BOS>'\n",
    "END_TAG = '<EOS>'\n",
    "\n",
    "class NERDataset_nonELMo(Dataset):\n",
    "    def __init__(self, sentences, labels, word_vocab: WordLevelVocab, labels_to_idx: dict, max_sentence_length: int = 15):\n",
    "        self.sentences = sentences\n",
    "        self.all_labels = labels\n",
    "        self.word_vocab = word_vocab\n",
    "        self.max_sentence_length = max_sentence_length\n",
    "        self.labels_to_idx = labels_to_idx\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sentences)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        sentence = self.sentences[idx]\n",
    "        # add padding and start and end tags to the sentence\n",
    "        labels = self.all_labels[idx]\n",
    "        return torch.tensor([self.word_vocab.word_to_index(word) for word in sentence]), torch.tensor(labels, dtype=torch.long)\n",
    "\n",
    "    def collate_fn(self, batch):\n",
    "        sentences, labels = zip(*batch)\n",
    "\n",
    "        bos_token = word_vocab.word_to_index(START_TAG)\n",
    "        eos_token = word_vocab.word_to_index(END_TAG)\n",
    "        pad_token = word_vocab.word_to_index(PAD_TAG)\n",
    "\n",
    "        sentences = [[bos_token] + list(sentence) + [eos_token] for sentence in sentences]\n",
    "        sentences = [sentence[:self.max_sentence_length] + [pad_token] * (self.max_sentence_length - len(sentence)) for sentence in sentences]\n",
    "\n",
    "        for i in range(len(sentences)):\n",
    "            sentences[i] = torch.tensor(sentences[i], dtype=torch.long)\n",
    "\n",
    "        # stack\n",
    "        sentences = torch.stack(sentences)\n",
    "\n",
    "        labels = [[labels_to_idx[START_TAG]] + list(label) + [labels_to_idx[END_TAG]] for label in labels]\n",
    "\n",
    "        labels = [label[:self.max_sentence_length] + [labels_to_idx[PAD_TAG]] * (self.max_sentence_length - len(label)) for label in labels]\n",
    "        labels = torch.tensor(labels, dtype=torch.long)\n",
    "\n",
    "        return sentences, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "ner_train_dataset_nonelmo = NERDataset_nonELMo(train_sentences, train_labels, word_vocab, labels_to_idx, max_sentence_length=15)\n",
    "ner_test_dataset_nonelmo = NERDataset_nonELMo(test_sentences, test_labels, word_vocab, labels_to_idx, max_sentence_length=15)\n",
    "ner_val_dataset_nonelmo = NERDataset_nonELMo(val_sentences, val_labels, word_vocab, labels_to_idx, max_sentence_length=15)\n",
    "\n",
    "ner_train_dataloader_nonelmo = DataLoader(ner_train_dataset_nonelmo, batch_size=32, shuffle=True, collate_fn=ner_train_dataset_nonelmo.collate_fn)\n",
    "ner_test_dataloader_nonelmo = DataLoader(ner_test_dataset_nonelmo, batch_size=32, shuffle=True, collate_fn=ner_test_dataset_nonelmo.collate_fn)\n",
    "ner_val_dataloader_nonelmo = DataLoader(ner_val_dataset_nonelmo, batch_size=32, shuffle=True, collate_fn=ner_val_dataset_nonelmo.collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([[     2,    117,     21,    867,   3708,   1522,     21,    151,    867,\n",
      "            687,      6,      3,      1,      1,      1],\n",
      "        [     2,   3044,  73660, 103346,   3044,   8569,   3020,      6,      3,\n",
      "              1,      1,      1,      1,      1,      1],\n",
      "        [     2,    331,   2630,     18,   7586,      8,   2633,   1468,     25,\n",
      "              3,      1,      1,      1,      1,      1],\n",
      "        [     2,  96836,   2465,    364,     75,    112,    349, 141599,   3338,\n",
      "           2242,   2825,      3,      1,      1,      1],\n",
      "        [     2,  10234,    661,   3902,      7,      0, 122782,   9347,    327,\n",
      "              3,      1,      1,      1,      1,      1],\n",
      "        [     2,    238,  10508,  10447,     82,  61258,   7257,     50,      6,\n",
      "              3,      1,      1,      1,      1,      1],\n",
      "        [     2,  74534, 125186,     30,      0,  44529,  64039,    158,    406,\n",
      "           1256,    171,   1951,      3,      1,      1],\n",
      "        [     2,    701,   8690,  10204,    878,     16,    892,   7039,  10269,\n",
      "           2615,   3534,  43429,      3,      1,      1],\n",
      "        [     2,  49926,    111,  26024,      8,  57902,  17717,     25,     19,\n",
      "              3,      1,      1,      1,      1,      1],\n",
      "        [     2,    300,  49943,    377,    115,     27,     63,      6,      3,\n",
      "              1,      1,      1,      1,      1,      1],\n",
      "        [     2,      0,     87,      0,   2150,      0,    542,     59,  18534,\n",
      "            125,     63,      6,      3,      1,      1],\n",
      "        [     2,  30879,     12,   2522,      7,      0,   7179,  36864,     13,\n",
      "              3,      1,      1,      1,      1,      1],\n",
      "        [     2,   1209,  32270,     14,    641,      0,      3,      1,      1,\n",
      "              1,      1,      1,      1,      1,      1],\n",
      "        [     2,   3957,   5459,      7,   5242,  65986,      3,      1,      1,\n",
      "              1,      1,      1,      1,      1,      1],\n",
      "        [     2,    105,  33956,  61258,   7257,   4782,   1620,    491,    194,\n",
      "              3,      1,      1,      1,      1,      1],\n",
      "        [     2,   1359,  11124,  27366,   1008,     19,      3,      1,      1,\n",
      "              1,      1,      1,      1,      1,      1],\n",
      "        [     2,  46464,  89131,   1045,  28715,      0,  18964,    607,     68,\n",
      "          30180,   1054,    117,     33,      3,      1],\n",
      "        [     2,   5661,    320,      0,    181,   4508,     54,      6,      3,\n",
      "              1,      1,      1,      1,      1,      1],\n",
      "        [     2,   2240,      0,   8287,      7,    540,    793,     51,     54,\n",
      "              6,      3,      1,      1,      1,      1],\n",
      "        [     2,   3197,     12,    661,     83,     31,      3,      1,      1,\n",
      "              1,      1,      1,      1,      1,      1],\n",
      "        [     2,   2146,    386,     48,  29154,   1644,   1551,     11,      3,\n",
      "              1,      1,      1,      1,      1,      1],\n",
      "        [     2,  29790,    156,     27,    129,    716,   1622,   2533,     16,\n",
      "            848,      3,      1,      1,      1,      1],\n",
      "        [     2,    527,    302,      0,   5941,   1502,     33,      3,      1,\n",
      "              1,      1,      1,      1,      1,      1],\n",
      "        [     2,      8,  16489,     39,   2723,    933,      6,      3,      1,\n",
      "              1,      1,      1,      1,      1,      1],\n",
      "        [     2,      0,    159,      0,     77,    243,     21,   2358,   5278,\n",
      "            143,     33,      3,      1,      1,      1],\n",
      "        [     2,    231,   5472,   4985,     16,      8,   2001,    749,     25,\n",
      "              6,      3,      1,      1,      1,      1],\n",
      "        [     2,  16122,    155,  48274,  11258,     46,   1451,   3891,    479,\n",
      "             30,      6,      3,      1,      1,      1],\n",
      "        [     2,      0,     43,    311,      0,     39,      0,  23367,   1363,\n",
      "            951,     74,    334,    951,      3,      1],\n",
      "        [     2,  46908,   2855,   2767,    671,  11613,   1767,   8467,      3,\n",
      "              1,      1,      1,      1,      1,      1],\n",
      "        [     2,   5579,   7648,  77046,    166,    100,    150,  30808,  39379,\n",
      "          26245,      6,      3,      1,      1,      1],\n",
      "        [     2,   6675,    300,    210,   1598,      0,   1528,    333,      3,\n",
      "              1,      1,      1,      1,      1,      1],\n",
      "        [     2,  18695,    993,   1066,    603,  18086,    191,    401,      3,\n",
      "              1,      1,      1,      1,      1,      1]]), tensor([[17, 13, 13, 13, 14, 13, 13, 13, 13, 13, 13, 16, 15, 15, 15],\n",
      "        [17, 13, 13, 13, 13, 13, 13, 13, 16, 15, 15, 15, 15, 15, 15],\n",
      "        [17,  6,  9, 13,  9, 13,  2, 13, 13, 16, 15, 15, 15, 15, 15],\n",
      "        [17, 13, 13, 13, 14, 13, 13, 13, 13, 13, 13, 16, 15, 15, 15],\n",
      "        [17,  0, 13, 13, 13,  0, 13, 13, 13, 16, 15, 15, 15, 15, 15],\n",
      "        [17,  7,  9,  5, 13, 13, 13, 13, 13, 16, 15, 15, 15, 15, 15],\n",
      "        [17, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 16, 15, 15],\n",
      "        [17,  2, 13,  9,  5, 13,  4, 11,  0,  0, 13, 13, 16, 15, 15],\n",
      "        [17, 13, 13, 13, 13, 13, 13, 13, 13, 16, 15, 15, 15, 15, 15],\n",
      "        [17, 13, 13, 13, 13, 13, 13, 13, 16, 15, 15, 15, 15, 15, 15],\n",
      "        [17,  2,  8,  8, 13, 13, 13, 13, 13, 13, 13, 13, 16, 15, 15],\n",
      "        [17,  9,  2,  8, 13,  2,  8, 13, 13, 16, 15, 15, 15, 15, 15],\n",
      "        [17, 13, 13, 13, 13, 13, 16, 15, 15, 15, 15, 15, 15, 15, 15],\n",
      "        [17, 13, 13, 13, 13, 13, 16, 15, 15, 15, 15, 15, 15, 15, 15],\n",
      "        [17,  6, 13, 13, 13, 13, 13, 13, 13, 16, 15, 15, 15, 15, 15],\n",
      "        [17, 13, 13, 13, 13, 13, 16, 15, 15, 15, 15, 15, 15, 15, 15],\n",
      "        [17, 13,  9,  3,  9,  9, 13, 13, 13, 13, 13, 13, 13, 16, 15],\n",
      "        [17, 11, 13,  2, 13, 13, 13, 13, 16, 15, 15, 15, 15, 15, 15],\n",
      "        [17, 13, 13,  4, 13,  4, 13, 13, 13, 13, 16, 15, 15, 15, 15],\n",
      "        [17, 13,  2, 13, 13, 13, 16, 15, 15, 15, 15, 15, 15, 15, 15],\n",
      "        [17, 13, 13, 13, 13, 13, 13, 13, 16, 15, 15, 15, 15, 15, 15],\n",
      "        [17, 13, 13, 13, 13, 13,  9,  5, 13, 13, 16, 15, 15, 15, 15],\n",
      "        [17, 13, 13, 13, 13, 13, 13, 16, 15, 15, 15, 15, 15, 15, 15],\n",
      "        [17, 13, 13, 13, 13, 13, 13, 16, 15, 15, 15, 15, 15, 15, 15],\n",
      "        [17, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 16, 15, 15, 15],\n",
      "        [17,  7,  9,  5, 13, 13, 13, 13, 13, 13, 16, 15, 15, 15, 15],\n",
      "        [17, 13,  0,  0, 13, 13, 13, 13, 13, 13, 13, 16, 15, 15, 15],\n",
      "        [17,  4, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 16, 15],\n",
      "        [17, 13, 13, 13, 13,  4, 13, 13, 16, 15, 15, 15, 15, 15, 15],\n",
      "        [17, 13, 13, 13, 13, 13, 13,  0, 13, 13, 13, 16, 15, 15, 15],\n",
      "        [17, 13, 13, 13, 13,  9, 13, 13, 16, 15, 15, 15, 15, 15, 15],\n",
      "        [17, 13, 13, 13, 13, 13, 13, 13, 16, 15, 15, 15, 15, 15, 15]]))\n"
     ]
    }
   ],
   "source": [
    "# check \n",
    "for batch in ner_train_dataloader_nonelmo:\n",
    "    print(batch)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NERModel_nonELMo(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim_1, hidden_dim_2, num_classes):\n",
    "        super(NERModel_nonELMo, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim_1, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim_1, hidden_dim_2)\n",
    "        self.output = nn.Linear(hidden_dim_2, num_classes)\n",
    "        self.non_linearity = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        embedding = self.embedding(x)\n",
    "        lstm_output, _ = self.lstm(embedding)\n",
    "        fc_output = self.fc(lstm_output.contiguous().view(-1, lstm_output.shape[2]))\n",
    "        fc_output = self.non_linearity(fc_output)\n",
    "        output = self.output(fc_output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ner_nonelmo = NERModel_nonELMo(word_vocab.num_words, 300, 150, 100, len(labels_to_idx))\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model_ner_nonelmo.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_nonelmo(model, train_dataloader, val_dataloader, optimizer, criterion, num_epochs, device):\n",
    "    model.to(device)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        predictions = []\n",
    "        true_labels = []\n",
    "        val_predictions = []\n",
    "        val_true_labels = []\n",
    "        loss_val = 0\n",
    "\n",
    "        for batch in tqdm(train_dataloader):\n",
    "            sentences, labels = batch\n",
    "            # print(type(sentences), type(labels))\n",
    "            sentences, labels = sentences.to(device), labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            output = model(sentences)\n",
    "\n",
    "            # print shapes\n",
    "            # print(f\"output shape: {output.shape}\")\n",
    "            # print(f\"labels shape: {labels.shape}\")\n",
    "\n",
    "            loss = criterion(output.view(-1, output.shape[-1]), labels.view(-1))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss += loss.item()\n",
    "            predictions.extend(output.argmax(dim=-1).view(-1).tolist())\n",
    "            true_labels.extend(labels.view(-1).tolist())\n",
    "\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for batch in tqdm(val_dataloader):\n",
    "                sentences, labels = batch\n",
    "                sentences, labels = sentences.to(device), labels.to(device)\n",
    "\n",
    "                output = model(sentences)\n",
    "\n",
    "                loss = criterion(output.view(-1, output.shape[-1]), labels.view(-1))\n",
    "                val_loss += loss.item()\n",
    "                val_predictions.extend(output.argmax(dim=-1).view(-1).tolist())\n",
    "                val_true_labels.extend(labels.view(-1).tolist())\n",
    "\n",
    "        print(f'Epoch: {epoch+1}/{num_epochs}, Train Loss: {train_loss/len(train_dataloader)}, Val Loss: {val_loss/len(val_dataloader)}')\n",
    "        wandb.log({'Train Loss': train_loss/len(train_dataloader), 'Val Loss': val_loss/len(val_dataloader)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:wdu7mpqx) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">my_run</strong> at: <a href='https://wandb.ai/project-ai-scream/my_project/runs/wdu7mpqx' target=\"_blank\">https://wandb.ai/project-ai-scream/my_project/runs/wdu7mpqx</a><br/> View project at: <a href='https://wandb.ai/project-ai-scream/my_project' target=\"_blank\">https://wandb.ai/project-ai-scream/my_project</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240508_165627-wdu7mpqx/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:wdu7mpqx). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.17.0 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home2/nanda.rajiv/inlp/project/new/INLP-Project-ELMo/tamil_ner/wandb/run-20240508_165633-skydijsk</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/project-ai-scream/INLP-Project-ELMo/runs/skydijsk' target=\"_blank\">Marathi without ELMo</a></strong> to <a href='https://wandb.ai/project-ai-scream/INLP-Project-ELMo' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/project-ai-scream/INLP-Project-ELMo' target=\"_blank\">https://wandb.ai/project-ai-scream/INLP-Project-ELMo</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/project-ai-scream/INLP-Project-ELMo/runs/skydijsk' target=\"_blank\">https://wandb.ai/project-ai-scream/INLP-Project-ELMo/runs/skydijsk</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 672/672 [00:08<00:00, 83.02it/s]\n",
      "100%|██████████| 47/47 [00:00<00:00, 261.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/10, Train Loss: 0.326531137738909, Val Loss: 0.19919749666401682\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 672/672 [00:07<00:00, 89.60it/s]\n",
      "100%|██████████| 47/47 [00:00<00:00, 325.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2/10, Train Loss: 0.16491283616051078, Val Loss: 0.16348176306866585\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 672/672 [00:08<00:00, 83.89it/s]\n",
      "100%|██████████| 47/47 [00:00<00:00, 341.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3/10, Train Loss: 0.11541456140167568, Val Loss: 0.15897054700775348\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 672/672 [00:07<00:00, 84.26it/s]\n",
      "100%|██████████| 47/47 [00:00<00:00, 342.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4/10, Train Loss: 0.08434223938855298, Val Loss: 0.17020990366631367\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 672/672 [00:07<00:00, 90.48it/s]\n",
      "100%|██████████| 47/47 [00:00<00:00, 330.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5/10, Train Loss: 0.0641193699224719, Val Loss: 0.18620026190864278\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 672/672 [00:07<00:00, 85.06it/s]\n",
      "100%|██████████| 47/47 [00:00<00:00, 323.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 6/10, Train Loss: 0.05187079091861267, Val Loss: 0.20222787051758867\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 672/672 [00:08<00:00, 82.83it/s]\n",
      "100%|██████████| 47/47 [00:00<00:00, 328.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 7/10, Train Loss: 0.04393549410728849, Val Loss: 0.21858685875826694\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 672/672 [00:08<00:00, 82.87it/s]\n",
      "100%|██████████| 47/47 [00:00<00:00, 326.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 8/10, Train Loss: 0.038990888118167366, Val Loss: 0.2360266572300424\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 672/672 [00:07<00:00, 84.02it/s]\n",
      "100%|██████████| 47/47 [00:00<00:00, 343.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 9/10, Train Loss: 0.03504767857832901, Val Loss: 0.25464376490166846\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 672/672 [00:08<00:00, 83.76it/s]\n",
      "100%|██████████| 47/47 [00:00<00:00, 285.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 10/10, Train Loss: 0.03325738032054644, Val Loss: 0.246055875686889\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Train Loss</td><td>█▄▃▂▂▁▁▁▁▁</td></tr><tr><td>Val Loss</td><td>▄▁▁▂▃▄▅▇█▇</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Train Loss</td><td>0.03326</td></tr><tr><td>Val Loss</td><td>0.24606</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">Marathi without ELMo</strong> at: <a href='https://wandb.ai/project-ai-scream/INLP-Project-ELMo/runs/skydijsk' target=\"_blank\">https://wandb.ai/project-ai-scream/INLP-Project-ELMo/runs/skydijsk</a><br/> View project at: <a href='https://wandb.ai/project-ai-scream/INLP-Project-ELMo' target=\"_blank\">https://wandb.ai/project-ai-scream/INLP-Project-ELMo</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240508_165633-skydijsk/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "config = {'model': 'No ELMo', 'language': 'Marathi', 'epochs': 10, 'batch_size': 32, 'lr': 0.001}\n",
    "wandb.init(project = 'INLP-Project-ELMo', group = 'NER', name = 'Marathi without ELMo', config = config)\n",
    "wandb.watch(model_ner_nonelmo)\n",
    "\n",
    "train_nonelmo(model_ner_nonelmo, ner_train_dataloader_nonelmo, ner_val_dataloader_nonelmo, optimizer, criterion, 10, device)\n",
    "\n",
    "wandb.join()\n",
    "torch.save(model_ner_nonelmo.state_dict(), 'ner_model_marathi_noelmo.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_ner_nonelmo(model, test_dataloader, device):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in test_dataloader:\n",
    "            sentences, labels = batch\n",
    "            sentences, labels = sentences.to(device), labels.to(device)\n",
    "\n",
    "            # flatten labels \n",
    "            labels = labels.view(-1)\n",
    "\n",
    "            output = model(sentences)\n",
    "            # print(f\"output shape: {output.shape}\")\n",
    "            # print(f\"labels shape: {labels.shape}\")\n",
    "\n",
    "            output = output.argmax(dim=-1)\n",
    "\n",
    "            correct += (output == labels).sum().item()\n",
    "            total += labels.numel()\n",
    "\n",
    "    accuracy = correct/total\n",
    "    return correct/total\n",
    "    # wandb.log({\"accuracy\": accuracy})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9542876209542877"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy = eval_ner_nonelmo(model_ner_nonelmo, ner_test_dataloader_nonelmo, device)\n",
    "accuracy"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "inlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
