{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\tanve\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torchtext\\vocab\\__init__.py:4: UserWarning: \n",
      "/!\\ IMPORTANT WARNING ABOUT TORCHTEXT STATUS /!\\ \n",
      "Torchtext is deprecated and the last released version will be 0.18 (this one). You can silence this warning by calling the following at the beginnign of your scripts: `import torchtext; torchtext.disable_torchtext_deprecation_warning()`\n",
      "  warnings.warn(torchtext._TORCHTEXT_DEPRECATION_MSG)\n",
      "c:\\Users\\tanve\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torchtext\\utils.py:4: UserWarning: \n",
      "/!\\ IMPORTANT WARNING ABOUT TORCHTEXT STATUS /!\\ \n",
      "Torchtext is deprecated and the last released version will be 0.18 (this one). You can silence this warning by calling the following at the beginnign of your scripts: `import torchtext; torchtext.disable_torchtext_deprecation_warning()`\n",
      "  warnings.warn(torchtext._TORCHTEXT_DEPRECATION_MSG)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from tqdm import tqdm\n",
    "from ELMO import ELMo\n",
    "import torch.nn.functional as F\n",
    "from wa_loader import WADataset, tokenize, split_into_characters\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load vocabularies\n",
    "vocab = torch.load('../hin_word_vocab.pt')\n",
    "character_vocab = torch.load('../hin_char_vocab.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# in the file Same.txt, append evey line that does not have ':' as the first non-space character\n",
    "same_sens = []\n",
    "with open('Same.txt', 'r', encoding='utf8') as f:\n",
    "    for line in f:\n",
    "        if line[0] != ' ' and line[0] != ':':\n",
    "            same_sens.append(line.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# same for Not_Same.txt\n",
    "not_same_sens = []\n",
    "with open('Not_Same.txt', 'r', encoding='utf8') as f:\n",
    "    for line in f:\n",
    "        if line[0] != ' ' and line[0] != ':':\n",
    "            not_same_sens.append(line.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "same_sens = tokenize(same_sens)\n",
    "not_same_sens = tokenize(not_same_sens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "jaise = \"जैसे\"\n",
    "to = \"तो\"\n",
    "waise = \"वैसे\"\n",
    "hi = \"ही\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for same_sens and not_same_sens, insert jaise at position 1, to at 3 and jaise at 5\n",
    "for i in range(len(same_sens)):\n",
    "    same_sens[i].insert(1, jaise)\n",
    "    same_sens[i].insert(3, waise)\n",
    "    same_sens[i].insert(4, hi)\n",
    "    same_sens[i].insert(6, jaise)\n",
    "for i in range(len(not_same_sens)):\n",
    "    not_same_sens[i].insert(1, jaise)\n",
    "    not_same_sens[i].insert(3, waise)\n",
    "    not_same_sens[i].insert(4, hi)\n",
    "    not_same_sens[i].insert(6, jaise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Sentences\n",
    "s1_dataset = WADataset(same_sens)\n",
    "s1, w1 = s1_dataset.format(character_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataloader(s1, w1, batch_size):\n",
    "    zipped = list(zip(s1, w1))\n",
    "    dataloader = DataLoader(zipped, batch_size=batch_size, shuffle=True)\n",
    "    return dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split s1 and w1 into train and test\n",
    "train_size = int(0.8 * len(s1))\n",
    "test_size = len(s1) - train_size\n",
    "train_s1, test_s1 = s1[:train_size], s1[train_size:]\n",
    "train_w1, test_w1 = w1[:train_size], w1[train_size:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[19], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mtrain_w1\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'list' object has no attribute 'shape'"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make the dataloader for modle with elmo\n",
    "\n",
    "batch_size = 128\n",
    "train_dataloader = create_dataloader(train_s1, train_w1, batch_size)\n",
    "val_dataloader = create_dataloader(test_s1, test_w1, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ELMo(cnn_config = {'character_embedding_size': 16, \n",
    "                           'num_filters': 32, \n",
    "                           'kernel_size': 5, \n",
    "                           'max_word_length': 10, \n",
    "                           'char_vocab_size': character_vocab.num_chars}, \n",
    "             elmo_config = {'num_layers': 3,\n",
    "                            'word_embedding_dim': 150,\n",
    "                            'vocab_size': vocab.num_words}, \n",
    "             char_vocab_size = character_vocab.num_chars).to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(torch.load('../model_elmo_hindi.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_embedding_dim = 300"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ELMo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import Adam\n",
    "\n",
    "class WordAnalogyModel(nn.Module):\n",
    "    def __init__(self, elmo):\n",
    "        super(WordAnalogyModel, self).__init__()\n",
    "        self.elmo = elmo\n",
    "        \n",
    "        # Freeze the parameters of the ELMo model since it's pretrained\n",
    "        for param in self.elmo.parameters():\n",
    "            param.requires_grad = False\n",
    "        \n",
    "        # You may want to include an additional layer or parameters for the task\n",
    "        # Example: A fully connected layer for the final prediction\n",
    "        # For simplicity, we will not add it here\n",
    "\n",
    "    def forward(self, sentence, target_word_index):\n",
    "        # Get the ELMo embeddings\n",
    "        _, _, final_embeddings = self.elmo(sentence)\n",
    "        \n",
    "        # Retrieve embeddings of the specified words\n",
    "        embedding_first = final_embeddings[0][0]  # First word\n",
    "        embedding_third = final_embeddings[0][2]  # Third word\n",
    "        embedding_sixth = final_embeddings[0][5]  # Sixth word\n",
    "        \n",
    "        # Perform analogy calculation: (first - third + sixth)\n",
    "        analogy_vector = embedding_first - embedding_third + embedding_sixth\n",
    "        \n",
    "        # Compute cosine similarity between analogy_vector and all word embeddings\n",
    "        # in the vocabulary, and find the most similar word index\n",
    "        similarities = F.cosine_similarity(analogy_vector.unsqueeze(0), final_embeddings[0], dim=1)\n",
    "        \n",
    "        # Find the index of the word with the highest similarity\n",
    "        predicted_word_index = torch.argmax(similarities)\n",
    "        \n",
    "        # Compute the loss with respect to the target word\n",
    "        loss = F.cross_entropy(similarities.unsqueeze(0), target_word_index.unsqueeze(0))\n",
    "        \n",
    "        return loss, predicted_word_index\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Without ELMo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# similarity_model = SimilarityModel(model)\n",
    "word_analogy_model = WordAnalogyModel(model).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 10\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(word_analogy_model.parameters(), lr=0.001)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, dataloader, optimizer, num_epochs=10):\n",
    "    model.train()  # Set the model to training mode\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        epoch_loss = 0.0  # Initialize loss for the epoch\n",
    "        \n",
    "        for i, batch in enumerate(dataloader):\n",
    "            # Assume batch is a tuple of (input_sentence, target_word)\n",
    "            input_sentence, target_word = batch\n",
    "            print(len(input_sentence[0]), len(target_word))\n",
    "            # Assuming `target_word` is an integer representing the index of the target word\n",
    "            target_word_index = target_word  # This should already be in the form of an index\n",
    "            \n",
    "            # Zero the gradients before forward pass\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Forward pass\n",
    "            loss, predicted_word_index = model(input_sentence, target_word_index)\n",
    "            \n",
    "            # Backward pass and optimization\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            # Accumulate loss for the epoch\n",
    "            epoch_loss += loss.item()\n",
    "        \n",
    "        # Compute average loss for the epoch\n",
    "        average_loss = epoch_loss / len(dataloader)\n",
    "        \n",
    "        # Print the average loss to monitor training progress\n",
    "        print(f\"Epoch {epoch + 1}/{num_epochs}, Loss: {average_loss:.4f}\")\n",
    "\n",
    "# Assuming you have an initialized `WordAnalogyModel` instance and an optimizer\n",
    "\n",
    "# Example usage:\n",
    "# optimizer = torch.optim.Adam(word_analogy_model.parameters(), lr=0.001)\n",
    "# train_model(word_analogy_model, dataloader, optimizer, num_epochs=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([[40,  7,  4,  ...,  0,  0,  0],\n",
      "        [20, 34, 15,  ...,  0,  0,  0],\n",
      "        [ 3,  6,  4,  ..., 18,  2,  0],\n",
      "        ...,\n",
      "        [13, 10,  9,  ...,  0,  0,  0],\n",
      "        [13,  7, 57,  ...,  0,  0,  0],\n",
      "        [31, 27, 10,  ...,  0,  0,  0]]), tensor([[21, 25,  9,  ...,  0,  0,  0],\n",
      "        [21, 25,  9,  ...,  0,  0,  0],\n",
      "        [21, 25,  9,  ...,  0,  0,  0],\n",
      "        ...,\n",
      "        [21, 25,  9,  ...,  0,  0,  0],\n",
      "        [21, 25,  9,  ...,  0,  0,  0],\n",
      "        [21, 25,  9,  ...,  0,  0,  0]]), tensor([[40,  7,  4,  ...,  0,  0,  0],\n",
      "        [20, 24, 15,  ...,  0,  0,  0],\n",
      "        [ 3,  6,  4,  ..., 18,  2, 50],\n",
      "        ...,\n",
      "        [13, 10,  9,  ...,  0,  0,  0],\n",
      "        [13, 10, 57,  ...,  0,  0,  0],\n",
      "        [31, 27, 10,  ...,  2,  0,  0]]), tensor([[19, 25,  9,  ...,  0,  0,  0],\n",
      "        [19, 25,  9,  ...,  0,  0,  0],\n",
      "        [19, 25,  9,  ...,  0,  0,  0],\n",
      "        ...,\n",
      "        [19, 25,  9,  ...,  0,  0,  0],\n",
      "        [19, 25,  9,  ...,  0,  0,  0],\n",
      "        [19, 25,  9,  ...,  0,  0,  0]]), tensor([[12,  7,  0,  ...,  0,  0,  0],\n",
      "        [12,  7,  0,  ...,  0,  0,  0],\n",
      "        [12,  7,  0,  ...,  0,  0,  0],\n",
      "        ...,\n",
      "        [12,  7,  0,  ...,  0,  0,  0],\n",
      "        [12,  7,  0,  ...,  0,  0,  0],\n",
      "        [12,  7,  0,  ...,  0,  0,  0]]), tensor([[ 3,  2, 18,  ...,  0,  0,  0],\n",
      "        [ 8,  2,  8,  ...,  0,  0,  0],\n",
      "        [22,  5, 15,  ...,  0,  0,  0],\n",
      "        ...,\n",
      "        [ 9,  6, 19,  ...,  0,  0,  0],\n",
      "        [ 9, 24, 11,  ...,  0,  0,  0],\n",
      "        [ 9, 24, 38,  ...,  0,  0,  0]]), tensor([[21, 25,  9,  ...,  0,  0,  0],\n",
      "        [21, 25,  9,  ...,  0,  0,  0],\n",
      "        [21, 25,  9,  ...,  0,  0,  0],\n",
      "        ...,\n",
      "        [21, 25,  9,  ...,  0,  0,  0],\n",
      "        [21, 25,  9,  ...,  0,  0,  0],\n",
      "        [21, 25,  9,  ...,  0,  0,  0]]), tensor([[ 3,  2, 18,  ...,  0,  0,  0],\n",
      "        [ 8,  2,  8,  ...,  0,  0,  0],\n",
      "        [22,  5, 15,  ...,  7,  0,  0],\n",
      "        ...,\n",
      "        [ 9,  6, 19,  ...,  0,  0,  0],\n",
      "        [ 9, 24, 11,  ...,  0,  0,  0],\n",
      "        [ 9, 24, 38,  ...,  0,  0,  0]])] tensor([[ 3,  2, 18,  ...,  0,  0,  0],\n",
      "        [ 8,  2,  8,  ...,  0,  0,  0],\n",
      "        [22,  5, 15,  ...,  7,  0,  0],\n",
      "        ...,\n",
      "        [ 9,  6, 19,  ...,  0,  0,  0],\n",
      "        [ 9, 24, 11,  ...,  0,  0,  0],\n",
      "        [ 9, 24, 38,  ...,  0,  0,  0]])\n"
     ]
    }
   ],
   "source": [
    "for s1, w1 in train_dataloader:\n",
    "    print(s1, w1)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "128 128\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'permute'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[31], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mword_analogy_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_epochs\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[30], line 18\u001b[0m, in \u001b[0;36mtrain_model\u001b[1;34m(model, dataloader, optimizer, num_epochs)\u001b[0m\n\u001b[0;32m     15\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m     17\u001b[0m \u001b[38;5;66;03m# Forward pass\u001b[39;00m\n\u001b[1;32m---> 18\u001b[0m loss, predicted_word_index \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_sentence\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_word_index\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;66;03m# Backward pass and optimization\u001b[39;00m\n\u001b[0;32m     21\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[1;32mc:\\Users\\tanve\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\tanve\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[21], line 21\u001b[0m, in \u001b[0;36mWordAnalogyModel.forward\u001b[1;34m(self, sentence, target_word_index)\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, sentence, target_word_index):\n\u001b[0;32m     20\u001b[0m     \u001b[38;5;66;03m# Get the ELMo embeddings\u001b[39;00m\n\u001b[1;32m---> 21\u001b[0m     _, _, final_embeddings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43melmo\u001b[49m\u001b[43m(\u001b[49m\u001b[43msentence\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     23\u001b[0m     \u001b[38;5;66;03m# Retrieve embeddings of the specified words\u001b[39;00m\n\u001b[0;32m     24\u001b[0m     embedding_first \u001b[38;5;241m=\u001b[39m final_embeddings[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m]  \u001b[38;5;66;03m# First word\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\tanve\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\tanve\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\tanve\\OneDrive\\Desktop\\INLP\\INLP-Project-ELMo-main\\ELMo\\word_anlogy\\ELMO.py:72\u001b[0m, in \u001b[0;36mELMo.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     68\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m     69\u001b[0m     \u001b[38;5;66;03m# character cnn\u001b[39;00m\n\u001b[0;32m     70\u001b[0m     \u001b[38;5;66;03m# convert x to tensor\u001b[39;00m\n\u001b[0;32m     71\u001b[0m     \u001b[38;5;66;03m# x = torch.stack(x, dim=0)\u001b[39;00m\n\u001b[1;32m---> 72\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpermute\u001b[49m(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m     73\u001b[0m     x \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchar_cnn(word) \u001b[38;5;28;01mfor\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m x]\n\u001b[0;32m     74\u001b[0m     \u001b[38;5;66;03m# lstm1\u001b[39;00m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'list' object has no attribute 'permute'"
     ]
    }
   ],
   "source": [
    "train_model(word_analogy_model, train_dataloader, optimizer, num_epochs=num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 20\n",
    "\n",
    "# train the model\n",
    "for epoch in range(num_epochs):\n",
    "    w.train()\n",
    "    total_loss = 0\n",
    "    tdiff = 0\n",
    "    for s1t, s2t, scorest in tqdm(train_dataloader_wo_elmo):\n",
    "        s1t = s1t.to(device)\n",
    "        s2t = s2t.to(device)\n",
    "        scorest = scorest.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        # print(len(s1t[0]))\n",
    "        # stack the sentences\n",
    "        # s1t = torch.stack(s1t, dim=1).to(device)\n",
    "        # s2t = torch.stack(s2t, dim=1).to(device)\n",
    "        outputs = similarity_model_wo_elmo(s1t, s2t)\n",
    "        # print(outputs.squeeze().shape, scores.shape)\n",
    "        # print(outputs.shape, scores.shape)\n",
    "        loss = criterion(outputs, scorest)\n",
    "        tdiff += torch.abs(outputs - scorest).sum().item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    print(f\"Epoch {epoch} Loss: {total_loss} Mean Difference: {tdiff/len(s1t)}\")\n",
    "\n",
    "    similarity_model.eval()\n",
    "    with torch.no_grad():\n",
    "        total_loss = 0\n",
    "        # calculate mean difference between predicted and actual scores\n",
    "        diff = 0\n",
    "\n",
    "        for s1v, s2v, scoresv in val_dataloader_wo_elmo:\n",
    "            s1v = s1v.to(device)\n",
    "            s2v = s2v.to(device)\n",
    "            scoresv = scoresv.to(device)\n",
    "            # s1v = torch.stack(s1v, dim=1).to(device)\n",
    "            # s2v = torch.stack(s2v, dim=1).to(device)\n",
    "            outputs = similarity_model_wo_elmo(s1v, s2v)\n",
    "            loss = criterion(outputs, scoresv)\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            diff += torch.abs(outputs - scoresv).sum().item()\n",
    "            # calculate \n",
    "\n",
    "        print(f\"Validation Loss: {total_loss}\" + f\" Mean Difference: {diff/len(s1v)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kitna = \"कितना\"\n",
    "milta = \"मिलता\"\n",
    "hai = \"है\"\n",
    "se = \"से\"\n",
    "bhookh = \"भूख\"\n",
    "bhookha = \"भूखा\"\n",
    "pyaas = \"प्यास\"\n",
    "imarat = \"इमारत\"\n",
    "\n",
    "mujhe = \"मुझे\"\n",
    "lag = \"लग\"\n",
    "rahi = \"रही\"\n",
    "\n",
    "aur = \"और\"\n",
    "\n",
    "# merge the words into a single string\n",
    "sentence = [mujhe + \" \" + bhookh + \" \"+ aur +\" \" + pyaas+\" \" + lag + \" \" +rahi + \" \" + hai]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = tokenize(sentence)\n",
    "t = split_into_characters(t, character_vocab, word_length=6, sen_len=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert t to a list of tensors\n",
    "for i in range(len(t)):\n",
    "    t[i] = torch.tensor(t[i]).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_tensor = torch.stack(t, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forward_output, backward_output, final_embeddings = model(sentence_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "e1 = final_embeddings[-1][2][0]\n",
    "e2 = final_embeddings[-1][6][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cosine_similarity = F.cosine_similarity(e1, e2, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
